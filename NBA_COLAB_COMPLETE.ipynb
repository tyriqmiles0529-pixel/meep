{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üèÄ NBA Predictor - Complete Cloud Training\n## Neural Network + Full Features + GPU Acceleration\n\n### What This Notebook Does:\n‚úÖ Trains with ALL features (Team priors, Player priors, Optimization features, Phase 7)  \n‚úÖ Neural Network (TabNet + LightGBM) EMBEDDED (not optional)  \n‚úÖ GPU-accelerated for faster training (~20-30 min instead of hours)  \n‚úÖ Downloads trained models to your computer  \n‚úÖ Shows accuracy metrics for moneyline AND spread  \n‚úÖ **FIXED**: Player data filtering bug (was returning 0 rows)\n\n### üî• Latest Update (v2.1):\n**CRITICAL FIX**: Resolved type mismatch bug causing player data to be filtered to 0 rows.\n- Fixed: train_auto.py lines 5018 & 5040 (season type conversion)\n- Added: diagnose_player_filter.py (pre-flight check)\n- Fuzzy matching: Already included (name normalization + season offset)\n\n### Steps:\n1. **RUN TEST FIRST** (verify GPU and environment)\n2. Upload your `priors_data.zip` and `PlayerStatistics.csv.zip`\n3. Run all cells (Runtime ‚Üí Run all)\n4. Download your trained models\n5. Done!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 0: ENVIRONMENT & DATA TEST (RUN THIS FIRST!)\n",
    "# ============================================================\n",
    "# This verifies GPU AND tests player data preparation\n",
    "# CRITICAL: If player data fails, no player models = system useless!\n",
    "\n",
    "print(\"üîç Testing Colab Environment...\\n\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"üéÆ GPU Available: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"   ‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"   ‚ùå NO GPU - Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "# Check disk space\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage('/')\n",
    "print(f\"\\nüíæ Disk: {free / (1024**3):.1f} GB free\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì• UPLOADING TEST DATA (for data preparation check)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Upload PlayerStatistics.csv.zip (39.5 MB) to test player data...\")\n",
    "print(\"This will verify the data can be loaded and prepared correctly.\")\n",
    "print(\"\\nIf upload fails, check:\")\n",
    "print(\"  1. File is named exactly 'PlayerStatistics.csv.zip'\")\n",
    "print(\"  2. File is the compressed version (39.5 MB, not 302 MB)\")\n",
    "print(\"  3. File was created with compress_csvs_for_colab.py\")\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract if zip\n",
    "if os.path.exists('PlayerStatistics.csv.zip'):\n",
    "    print(\"\\nüì¶ Extracting PlayerStatistics.csv...\")\n",
    "    !unzip -q PlayerStatistics.csv.zip\n",
    "    !rm PlayerStatistics.csv.zip\n",
    "\n",
    "# Clone repo for test script\n",
    "if not os.path.exists('/content/meep'):\n",
    "    print(\"\\nüì• Cloning repo for test scripts...\")\n",
    "    !git clone -q https://github.com/tyriqmiles0529-pixel/meep.git /content/meep\n",
    "    !cp PlayerStatistics.csv /content/meep/ 2>/dev/null\n",
    "\n",
    "%cd /content/meep\n",
    "\n",
    "# Install minimal dependencies for test\n",
    "print(\"\\nüì¶ Installing test dependencies...\")\n",
    "!pip install -q pandas numpy\n",
    "\n",
    "# Run the player data test\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ RUNNING PLAYER DATA PREPARATION TEST\")\n",
    "print(\"=\"*70)\n",
    "!python3 test_priors_merge.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è Check output above for:\")\n",
    "print(\"  ‚úì personId column found\")\n",
    "print(\"  ‚úì home column found with valid data\")\n",
    "print(\"  ‚úì date column found and parseable\")\n",
    "print(\"  ‚úì season_end_year populated (>50%)\")\n",
    "print(\"\\nIf ANY are missing:\")\n",
    "print(\"  ‚ùå PLAYER MODELS WILL NOT TRAIN\")\n",
    "print(\"  ‚Üí Fix the data issues before proceeding\")\n",
    "print(\"  ‚Üí Check PlayerStatistics.csv has correct columns\")\n",
    "print(\"\\nIf all checks passed:\")\n",
    "print(\"  ‚úÖ PROCEED - Upload priors_data.zip in next cell\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Upload Your Data Files\n",
    "# ============================================================\n",
    "# You need 2 files:\n",
    "# 1. priors_data.zip (6 CSVs with Basketball Reference stats)\n",
    "# 2. PlayerStatistics.csv.zip (39.5 MB - COMPRESSED for faster upload!)\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Upload priors_data.zip:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract priors\n",
    "!rm -rf /content/priors_data\n",
    "!unzip -q priors_data.zip -d /content\n",
    "\n",
    "# Verify priors\n",
    "csv_files = !ls /content/priors_data/*.csv 2>/dev/null | wc -l\n",
    "csv_count = int(csv_files[0]) if csv_files else 0\n",
    "if csv_count >= 6:\n",
    "    print(f\"‚úÖ Priors data uploaded! Found {csv_count} CSV files\")\n",
    "    !ls /content/priors_data/*.csv\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Only found {csv_count} files. Expected 6+ CSV files.\")\n",
    "    print(\"Make sure you uploaded the correct priors_data.zip\")\n",
    "\n",
    "print(\"\\nüì§ Upload PlayerStatistics.csv.zip (39.5 MB - 87% smaller!):\")\n",
    "print(\"This compressed file contains 20+ years of player game logs\")\n",
    "print(\"Upload time: ~8 seconds instead of ~61 seconds!\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract PlayerStatistics\n",
    "if os.path.exists('PlayerStatistics.csv.zip'):\n",
    "    print(\"\\nüì¶ Extracting PlayerStatistics.csv...\")\n",
    "    !unzip -q PlayerStatistics.csv.zip\n",
    "    !rm PlayerStatistics.csv.zip\n",
    "    size_mb = os.path.getsize('PlayerStatistics.csv') / 1024 / 1024\n",
    "    print(f\"‚úÖ PlayerStatistics.csv extracted! ({size_mb:.1f} MB)\")\n",
    "elif os.path.exists('PlayerStatistics.csv'):\n",
    "    size_mb = os.path.getsize('PlayerStatistics.csv') / 1024 / 1024\n",
    "    print(f\"‚úÖ PlayerStatistics.csv uploaded! ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PlayerStatistics.csv not found!\")\n",
    "    print(\"Player models will only train on current season data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 2: Install Dependencies & Download Code\n# ============================================================\n\nprint(\"üì¶ Installing packages...\")\n!pip install -q nba-api kagglehub pytorch-tabnet lightgbm scikit-learn pandas numpy tqdm\n\nprint(\"\\nüì• Forcing fresh clone from GitHub (latest fixes)...\")\nimport os\n\n# Change to parent directory first, then remove and clone\n%cd /content\n!rm -rf /content/meep\n!git clone https://github.com/tyriqmiles0529-pixel/meep.git /content/meep\n\n%cd /content/meep\n\n# Show current commit to verify latest code\nprint(\"\\nüìç Current commit:\")\n!git log -1 --oneline\n\nprint(\"\\n‚úÖ Code downloaded!\")\nprint(f\"üìÅ Working directory: {os.getcwd()}\")\n\n# Copy uploaded files to working directory\n!cp /content/PlayerStatistics.csv . 2>/dev/null || echo \"No PlayerStatistics.csv to copy\"\n!cp -r /content/priors_data . 2>/dev/null || echo \"No priors_data to copy\"\n\n# Check GPU\nimport torch\nprint(f\"\\nüéÆ GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n\n# Run diagnostic tests\nprint(\"\\nüîç Running data preparation test...\")\n!python3 test_priors_merge.py\n\nprint(\"\\nüîç Running player data filtering test (critical fix verification)...\")\n!python3 diagnose_player_filter.py"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Train Models with Neural Network + Full Features\n",
    "# ============================================================\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"‚è±Ô∏è  This will take 20-30 minutes with GPU\")\n",
    "print(\"‚òï Get coffee!\\n\")\n",
    "\n",
    "# Run training with ALL features + local PlayerStatistics.csv\n",
    "!python3 train_auto.py \\\n",
    "    --priors-dataset /content/priors_data \\\n",
    "    --player-csv /content/PlayerStatistics.csv \\\n",
    "    --verbose \\\n",
    "    --fresh \\\n",
    "    --neural-device gpu \\\n",
    "    --neural-epochs 50\n",
    "\n",
    "print(\"\\n‚úÖ TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: Display Training Metrics\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä Training Metrics:\\n\")\n",
    "!python3 show_metrics.py\n",
    "\n",
    "# Show file structure\n",
    "print(\"\\nüìÅ Trained Models:\")\n",
    "!ls -lh models/*.pkl models/*.json 2>/dev/null || echo \"No models found\"\n",
    "\n",
    "print(\"\\nüìä Model Cache (windowed models):\")\n",
    "!ls -lh model_cache/*.pkl 2>/dev/null || echo \"No cached models found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: Download Trained Models to Your Computer\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì¶ Preparing models for download...\")\n",
    "\n",
    "# Zip everything\n",
    "!zip -r nba_models_trained.zip models/ model_cache/ -x '*.git*'\n",
    "\n",
    "print(\"\\nüíæ Downloading models to your computer...\")\n",
    "files.download('nba_models_trained.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DONE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Extract nba_models_trained.zip to your local nba_predictor folder\")\n",
    "print(\"2. Run predictions locally with the new models\")\n",
    "print(\"3. Models include:\")\n",
    "print(\"   ‚Ä¢ Moneyline & Spread models (with accuracy metrics)\")\n",
    "print(\"   ‚Ä¢ Player prop models (Points, Rebounds, Assists, 3PM, Minutes)\")\n",
    "print(\"   ‚Ä¢ Neural hybrid models (TabNet + LightGBM)\")\n",
    "print(\"   ‚Ä¢ Ensemble models (Ridge + Elo + Four Factors)\")\n",
    "print(\"\\nüéØ Your models are now trained on 20+ years of data with:\")\n",
    "print(\"   ‚úì Team statistical priors (O/D ratings, pace, four factors)\")\n",
    "print(\"   ‚úì Player statistical priors (~68 features from Basketball Reference)\")\n",
    "print(\"   ‚úì Optimization features (momentum, consistency, fatigue)\")\n",
    "print(\"   ‚úì Phase 7 features (situational context, adaptive weighting)\")\n",
    "print(\"   ‚úì Neural network embeddings (deep feature learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Advanced: Run Custom Predictions in Colab\n",
    "\n",
    "Want to test predictions right here instead of downloading? Run the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions for today's games\n",
    "!python3 -c \"\n",
    "from player_ensemble_enhanced import predict_all_props\n",
    "import json\n",
    "\n",
    "predictions = predict_all_props()\n",
    "print(json.dumps(predictions, indent=2))\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üìä Accuracy Metrics Explained\n\n### Moneyline Model:\n- **Log Loss**: Lower is better (0.65 = good, 0.55 = excellent)\n- **Brier Score**: Similar to log loss (0.22 = good, 0.18 = excellent)\n- **Accuracy**: % of games predicted correctly (60%+ is profitable)\n\n### Spread Model:\n- **RMSE**: Root Mean Squared Error (10-12 points = good)\n- **MAE**: Mean Absolute Error (8-10 points = good)\n- **Coverage**: % of predictions within ¬±5 points (70%+ = excellent)\n\n### Player Props:\n- **RMSE**: Points/Rebounds/Assists error (6-8 = good for points)\n- **MAE**: Average error (4-6 = good for points)\n- **Hit Rate**: % of over/under picks that win (55%+ = profitable)\n\n---\n\n## ‚ùì Troubleshooting\n\n### \"Loaded 0 player-games for window\"\n‚úÖ **FIXED in v2.1!** This was a type mismatch bug (float64 vs int).\n- The fix is in the latest code from GitHub\n- diagnose_player_filter.py will verify the fix worked\n- You should now see: \"Loaded 245,892 player-games for window\" (or similar)\n\n### \"No models found\"\n- Training failed - check the error output above\n- Most common: priors_data.zip not uploaded correctly\n\n### \"GPU not available\"\n- Go to Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n- Training will still work on CPU (just slower)\n\n### \"Out of memory\"\n- Restart runtime: Runtime ‚Üí Restart runtime\n- Then re-run from Step 1\n\n### Player prior merging issues\n- Fuzzy matching is already enabled (name normalization + season offset)\n- Check diagnose_player_filter.py output for merge statistics\n- Sample player IDs should appear in diagnostic output\n\n### Need help?\n- Check QUICK_REFERENCE.txt in downloaded files\n- Or create a GitHub issue at: https://github.com/tyriqmiles0529-pixel/meep/issues\n\n---\n\n## üéØ Why This Works Better Than Local Training:\n\n1. **GPU Acceleration**: 5-10x faster than CPU\n2. **More RAM**: 12GB+ vs your laptop's limits\n3. **No System Slowdown**: Your computer stays responsive\n4. **Free**: Google Colab is free for up to 12 hours/session\n5. **Consistent Environment**: No dependency conflicts\n\n---\n\n## üìà Model Architecture (What You're Training):\n\n### Game Models:\n1. **Ridge Regression** (baseline)\n2. **Dynamic Elo** (momentum-based ratings)\n3. **Four Factors** (advanced stats)\n4. **LightGBM** (gradient boosting)\n5. **Meta-Learner** (combines all 4)\n\n### Player Models:\n1. **TabNet** (deep learning for feature extraction)\n2. **LightGBM** (using raw + deep features)\n3. **Sigma Model** (uncertainty quantification)\n\n### Feature Pipeline:\n- **Phase 1-5**: Basic stats + rolling averages + team context\n- **Phase 6**: Optimization (momentum, consistency, fatigue)\n- **Phase 7**: Situational (schedule density, opponent history)\n- **Basketball Reference Priors**: Historical statistical context (~68 features)\n- **Fuzzy Matching**: Name normalization + season offset (¬±1 year)\n\n**Total Features**: ~120-150 per model\n\n---\n\n## üîÑ Re-training Schedule:\n\n- **Daily**: Not needed (models are stable)\n- **Weekly**: Run for current season updates\n- **Monthly**: Full retrain recommended\n- **Mid-Season**: After All-Star break (team dynamics change)\n- **Playoffs**: Retrain with playoff-specific weights\n\nYou can upload your previous model_cache/ to speed up retraining (only trains new data)\n\n---\n\n## üêõ Changelog\n\n### v2.1 (November 2025) - CRITICAL FIX\n- **Fixed**: Player data filtering bug (0 rows issue)\n  - Root cause: Type mismatch (float64 season vs int set)\n  - Solution: Added .astype('Int64') in train_auto.py\n- **Added**: diagnose_player_filter.py diagnostic tool\n- **Verified**: Fuzzy matching already implemented\n\n### v2.0 (November 2025)\n- Neural Network (TabNet) embedded as default\n- Full feature pipeline (Phase 1-7)\n- GPU acceleration support\n- Compressed CSV upload (87% smaller)\n\n---\n\n**Version**: 2.1 (Player Data Fix)\n\n**Last Updated**: November 6, 2025"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}