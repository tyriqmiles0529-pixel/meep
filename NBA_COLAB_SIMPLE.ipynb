{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NBA] NBA Predictor - Neural Hybrid Training (Game + Player Models)\n\n## Features:\n- [x] **FULL NBA HISTORY**: 1947-2026 (80 seasons, 1.6M player-games)\n- [x] **Neural hybrid: TabNet + LightGBM** with 24-dim embeddings\n- [x] Basketball Reference priors: ~68 advanced stats (already merged!)\n- [x] **Game Models**: Moneyline + Spread predictions\n- [x] **Player Models**: Minutes, Points, Rebounds, Assists, Threes\n- [x] **Phase 1-7 features**: Built automatically during training\n\n## Quick Start:\n1. **Add \"meeper\" dataset** in Kaggle (Add Data -> search \"meeper\")\n2. **Enable GPU** (P100 or T4 x2)\n3. **Run Cell 1** (setup - 2 min)\n4. **Run Cell 2** (training - 7-8 hours)\n5. **Close browser** - Kaggle keeps running!\n6. **Come back later** and download models\n\n**GPU Required:** P100 (best) or T4 x2\n\n## What You Get:\n- **Game Models** (2): Moneyline classifier + Spread regressor\n- **Player Models** (5): Minutes, Points, Rebounds, Assists, Threes\n- **All with 24-dim embeddings** from TabNet + LightGBM\n- **Uncertainty quantification** via sigma models\n\n## Training Time:\n- **P100**: ~7 hours\n- **T4**: ~8 hours\n- Feature building: 90 min\n- Game models: 1 hour  \n- Player models: 5 hours\n\n## Expected Performance:\n- **Game accuracy**: 63.5-64.5% (beats Vegas 52.4% vig)\n- **Points MAE**: ~2.0-2.1 (22% better than baseline 2.65)\n- **Embeddings**: 24-dimensional, 15-40% feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SETUP (Kaggle Version)\n# ============================================================\n\nprint(\"Installing packages...\")\n!pip install -q pytorch-tabnet lightgbm scikit-learn pandas numpy tqdm\n\nprint(\"\\nDownloading training code from GitHub...\")\nimport os\n\n# Navigate to Kaggle working directory\nos.chdir('/kaggle/working')\n\n# Remove old clone if exists (to get latest code)\nif os.path.exists('nba_predictor'):\n    import shutil\n    shutil.rmtree('nba_predictor')\n    print(\"Removed old code - getting latest version\")\n\n# Clone your repository (gets latest version with --hybrid-player flag)\n!git clone https://github.com/tyriqmiles0529-pixel/meep.git nba_predictor\nos.chdir('nba_predictor')\n\nprint(\"\\nCode version:\")\n!git log -1 --oneline\n\n# Check GPU\nimport torch\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'\nprint(f\"\\nGPU: {gpu_name}\")\n\n# Verify dataset exists (added via \"Add Data\" in Kaggle UI)\ndataset_path = '/kaggle/input/meeper/aggregated_nba_data.csv/aggregated_nba_data.csv.gzip'\nif os.path.exists(dataset_path):\n    size_mb = os.path.getsize(dataset_path) / 1024 / 1024\n    print(f\"\\nDataset found: {size_mb:.1f} MB\")\n    print(f\"   Path: {dataset_path}\")\n    print(f\"   Full NBA history: 1947-2026 (80 seasons, 1.6M player-games)\")\n    print(f\"   Training will use: ALL DATA (no cutoff)\")\nelse:\n    print(\"\\nDataset not found!\")\n    print(\"   Make sure you added 'meeper' dataset to this notebook\")\n    print(\"   Click 'Add Data' -> search 'meeper' -> Add\")\n\nprint(\"\\nSetup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAIN NEURAL HYBRID MODELS - GAME + PLAYER (HYBRID MODE)\n# ============================================================\n\nimport os\n\n# Make sure we're in the code directory\nif not os.path.exists('/kaggle/working/nba_predictor'):\n    print(\"ERROR: Repository not found!\")\n    print(\"Run Cell 1 first to clone the repository\")\n    raise FileNotFoundError(\"Repository directory /kaggle/working/nba_predictor does not exist\")\n\nos.chdir('/kaggle/working/nba_predictor')\n\nprint(\"=\"*70)\nprint(\"NBA HYBRID MULTI-TASK TRAINING - 3X FASTER + MEMORY SAFE\")\nprint(\"=\"*70)\n\nprint(\"\\n[DATA] Dataset Info:\")\nprint(\"   Source: /kaggle/input/meeper/aggregated_nba_data.csv.gzip\")\nprint(\"   Mode: MEMORY-SAFE (chunked loading + 2002+ filter)\")\nprint(\"   Range: 2002-2026 (24 seasons, ~320K player-games)\")\nprint(\"   Priors: Basketball Reference stats ALREADY MERGED (68 features)\")\nprint(\"\\n[ROCKET] HYBRID MULTI-TASK MODE (3x FASTER):\")\nprint(\"   1. Load data in chunks with 2002+ filter (30 sec)\")\nprint(\"   2. Build Phase 1-6 features (30 min)\")\nprint(\"   3. Train GAME models (30 min)\")\nprint(\"      - Moneyline: TabNet + LightGBM\")\nprint(\"      - Spread: TabNet + LightGBM\")\nprint(\"   4. Train PLAYER models - HYBRID (1 hour)\")\nprint(\"      - Multi-task: Points/Assists/Rebounds (shared TabNet)\")\nprint(\"      - Single-task: Minutes/Threes (separate TabNets)\")\nprint(\"\\n[GEAR] Architecture:\")\nprint(\"   Game Models: Neural hybrid (TabNet + LightGBM)\")\nprint(\"   Player Models: HYBRID multi-task (correlated + independent)\")\n\nimport torch\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n\nif 'P100' in gpu_name:\n    print(\"\\n[TIMER] Expected time: ~2 hours total (P100)\")\nelif 'T4' in gpu_name:\n    print(\"\\n[TIMER] Expected time: ~2.5 hours total (T4)\")\nelse:\n    print(\"\\n[TIMER] Expected time: ~2-2.5 hours (GPU)\")\n\nprint(\"\\n[TARGET] Models to train:\")\nprint(\"   Game: Moneyline + Spread (neural hybrid)\")\nprint(\"   Player Multi-task: Points/Assists/Rebounds (shared TabNet)\")\nprint(\"   Player Single-task: Minutes, Threes (separate TabNets)\")\nprint(\"\\n[LIGHTNING] Why hybrid multi-task for players?\")\nprint(\"   - Points/Assists/Rebounds are correlated (usage, playstyle)\")\nprint(\"   - Minutes/Threes are independent (rotation, shot selection)\")\nprint(\"   - 1 shared TabNet for 3 props + 2 separate = 3x faster!\")\nprint(\"\\n[SHIELD] Memory Safety:\")\nprint(\"   - Chunked loading: Loads 100K rows at a time, filters, discards\")\nprint(\"   - Never holds full 1.6M dataset in memory\")\nprint(\"   - Peak RAM: ~2-3GB (well within Kaggle's 13GB limit)\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"STARTING TRAINING...\")\nprint(\"=\"*70 + \"\\n\")\n\n# Run training - HYBRID MODE with CHUNKED LOADING (memory-safe)\n!python train_auto.py \\\n    --aggregated-data /kaggle/input/meeper/aggregated_nba_data.csv/aggregated_nba_data.csv.gzip \\\n    --game-neural \\\n    --hybrid-player \\\n    --memory-limit \\\n    --neural-epochs 50 \\\n    --batch-size 4096 \\\n    --neural-device auto \\\n    --verbose\n\n# MEMORY-SAFE MODE (--memory-limit):\n# - Chunked loading: Reads CSV in 100K row chunks\n# - Early filtering: Keeps only 2002+ data (discards 80% BEFORE loading)\n# - Result: 320K rows instead of 1.6M, ~2GB RAM instead of 12GB\n# - Trade-off: Loses pre-2002 data, but still 24 seasons (excellent coverage)\n#\n# HYBRID MODE FLAGS:\n# --aggregated-data: Pre-aggregated CSV with priors ALREADY merged\n# --game-neural: Use TabNet + LightGBM for game models (better accuracy)\n# --hybrid-player: Hybrid multi-task for player props (3x faster!)\n#   - Multi-task: Points/Assists/Rebounds (shared TabNet)\n#   - Single-task: Minutes/Threes (separate TabNets)\n#\n# --memory-limit: CRITICAL for Kaggle - prevents OOM crashes\n# --neural-epochs 50: 50 epochs for TabNet (correlated props)\n# --batch-size 4096: Large batches for GPU efficiency\n# --neural-device auto: Auto-detect GPU\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\n[PACKAGE] Models saved to: /kaggle/working/nba_predictor/models/\")\nprint(\"\\nGame models:\")\nprint(\"  [x] moneyline_model.pkl (neural hybrid)\")\nprint(\"  [x] spread_model.pkl (neural hybrid)\")\nprint(\"\\nPlayer models:\")\nprint(\"  [x] hybrid_player_model.pkl (all 5 props in one model)\")\nprint(\"  [x] Individual wrappers:\")\nprint(\"      - points_model.pkl\")\nprint(\"      - assists_model.pkl\")\nprint(\"      - rebounds_model.pkl\")\nprint(\"      - minutes_model.pkl\")\nprint(\"      - threes_model.pkl\")\nprint(\"\\nNext: Run validation cell to check embeddings\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# VALIDATE 24-DIM EMBEDDINGS\n# ============================================================\n\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# Navigate to code directory\nos.chdir('/kaggle/working/nba_predictor')\n\nprint(\"* Validating TabNet embeddings...\\n\")\n\nmodels_dir = Path('./models')\n\n# Check if models directory exists\nif not models_dir.exists():\n    print(\"[X] Models directory not found!\")\n    print(\"   Expected: /kaggle/working/nba_predictor/models/\")\n    print(\"   Run training cell first\")\nelse:\n    # Find points model\n    model_files = list(models_dir.glob('*points*.pkl'))\n    \n    if not model_files:\n        print(\"[X] No points model found!\")\n        print(f\"   Searching in: {models_dir.absolute()}\")\n        print(f\"   Files found: {list(models_dir.glob('*.pkl'))}\")\n    else:\n        model_path = model_files[0]\n        print(f\"* Loading model: {model_path.name}\")\n        \n        with open(model_path, 'rb') as f:\n            model = pickle.load(f)\n        \n        print(f\"   Model type: {type(model).__name__}\")\n        \n        # Check if it's a NeuralHybridPredictor\n        if hasattr(model, 'tabnet'):\n            print(f\"   [x] Neural hybrid detected\")\n            print(f\"   TabNet: {type(model.tabnet).__name__}\")\n            print(f\"   LightGBM: {type(model.lgbm).__name__}\")\n            \n            # Test embedding extraction\n            print(\"\\nðŸ§ª Testing embedding extraction...\")\n            \n            # Create dummy data (match actual feature count)\n            n_features = 150  # Adjust if needed\n            dummy_features = pd.DataFrame(\n                np.random.randn(10, n_features),\n                columns=[f'feature_{i}' for i in range(n_features)]\n            )\n            \n            # Get embeddings\n            try:\n                if hasattr(model, '_get_embeddings'):\n                    embeddings = model._get_embeddings(dummy_features)\n                    print(f\"\\n[x] SUCCESS!\")\n                    print(f\"   Embedding shape: {embeddings.shape}\")\n                    print(f\"   Expected: (10, 24)\")\n                    \n                    if embeddings.shape[1] == 24:\n                        print(f\"\\n* PERFECT: Got 24-dimensional embeddings\")\n                        print(f\"   Mean: {embeddings.mean():.4f}\")\n                        print(f\"   Std: {embeddings.std():.4f}\")\n                        \n                        # Check LightGBM uses embeddings\n                        if hasattr(model.lgbm, 'feature_name_'):\n                            lgbm_features = model.lgbm.feature_name_\n                            embedding_features = [f for f in lgbm_features if 'embedding' in f.lower()]\n                            print(f\"   LightGBM sees {len(embedding_features)} embedding features\")\n                        \n                        print(f\"\\n[x] Model validation PASSED!\")\n                        print(f\"   Ready for predictions\")\n                    else:\n                        print(f\"\\nâš ï¸  Warning: Got {embeddings.shape[1]}-dim embeddings\")\n                else:\n                    print(f\"âš ï¸  Model doesn't have _get_embeddings method\")\n            \n            except Exception as e:\n                print(f\"[X] Error: {e}\")\n                import traceback\n                traceback.print_exc()\n        else:\n            print(f\"   âš ï¸  LightGBM-only model (no neural hybrid)\")\n        \n        # Display model info\n        print(f\"\\n[DATA] Model Summary:\")\n        if hasattr(model, 'lgbm'):\n            print(f\"   LightGBM trees: {model.lgbm.n_estimators if hasattr(model.lgbm, 'n_estimators') else 'N/A'}\")\n            if hasattr(model.lgbm, 'feature_name_'):\n                print(f\"   Features used: {len(model.lgbm.feature_name_)}\")\n        \n        if hasattr(model, 'sigma_model'):\n            print(f\"   Uncertainty model: {'Yes' if model.sigma_model else 'No'}\")\n\nprint(\"\\n[x] Validation complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAINING RESULTS SUMMARY\n# ============================================================\n\nimport os\nfrom pathlib import Path\n\nos.chdir('/kaggle/working/nba_predictor')\n\nmodels_dir = Path('./models')\n\nprint(\"=\"*70)\nprint(\"TRAINING RESULTS\")\nprint(\"=\"*70)\n\nif not models_dir.exists():\n    print(\"\nNo models directory found!\")\nelse:\n    model_files = list(models_dir.glob('*.pkl'))\n    \n    if not model_files:\n        print(\"\nNo models found!\")\n        print(f\"   Directory: {models_dir.absolute()}\")\n    else:\n        print(f\"\nFound {len(model_files)} trained models:\n\")\n        \n        for model_path in sorted(model_files):\n            print(f\"   {model_path.name}\")\n            size_mb = model_path.stat().st_size / 1024 / 1024\n            print(f\"      Size: {size_mb:.1f} MB\")\n            \n            # Try to load and check type\n            try:\n                import pickle\n                with open(model_path, 'rb') as f:\n                    model = pickle.load(f)\n                \n                if hasattr(model, 'tabnet'):\n                    print(f\"      Type: Neural Hybrid\")\n                else:\n                    print(f\"      Type: LightGBM only\")\n                \n                if hasattr(model, 'sigma_model') and model.sigma_model:\n                    print(f\"      Uncertainty: Yes\")\n                \n                print()\n            except Exception as e:\n                print(f\"      Error loading: {e}\n\")\n\nprint(\"=\"*70)\nprint(\"\nModels location: /kaggle/working/nba_predictor/models/\")\nprint(\"\nNext: Package and download models\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# PACKAGE MODELS FOR DOWNLOAD\n# ============================================================\n\nimport os\n\nos.chdir('/kaggle/working')\n\nprint(\"* Packaging models...\")\n\n# Check if models exist\nif not os.path.exists('nba_predictor/models'):\n    print(\"\\n[X] No models directory found!\")\n    print(\"   Run training first\")\nelse:\n    # Create zip file\n    !zip -r nba_models_trained.zip nba_predictor/models/ nba_predictor/model_cache/ 2>/dev/null\n    \n    # Check if zip was created\n    if os.path.exists('nba_models_trained.zip'):\n        size_mb = os.path.getsize('nba_models_trained.zip') / 1024 / 1024\n        print(f\"\\n[x] Package created: nba_models_trained.zip ({size_mb:.1f} MB)\")\n        print(\"\\nðŸ“¥ To download:\")\n        print(\"   1. Look at the right sidebar\")\n        print(\"   2. Click 'Output' tab\")\n        print(\"   3. Find 'nba_models_trained.zip'\")\n        print(\"   4. Click the download icon (â†“)\")\n        print(\"\\nOr use this command to download via Kaggle API:\")\n        print(\"   (This requires Kaggle notebook to be public)\")\n    else:\n        print(\"\\n[X] Failed to create zip file\")\n        print(\"   Check if models exist in nba_predictor/models/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## ðŸ“‹ What You Get After Training\n\n### Game Models (2 files):\n- `moneyline_ensemble_1947_2026.pkl` - Win probability predictions\n- `spread_ensemble_1947_2026.pkl` - Margin predictions\n- Both with TabNet + LightGBM ensemble architecture\n- Expected accuracy: 63.5-64.5%\n\n### Player Models (5 files):\n- `minutes_hybrid_1947_2026.pkl`\n- `points_hybrid_1947_2026.pkl`\n- `rebounds_hybrid_1947_2026.pkl`\n- `assists_hybrid_1947_2026.pkl`\n- `threes_hybrid_1947_2026.pkl`\n- All with 24-dimensional TabNet embeddings + LightGBM\n- Points MAE: ~2.0-2.1 (22% improvement over baseline)\n\n### Features Included:\n- **Raw stats**: 40 box score features from 1947-2026\n- **Basketball Reference priors**: 68 advanced stats (already merged!)\n- **Phase 1-6 features**: 150+ engineered features (built during training)\n- **Total**: ~235 features per prediction\n\n---\n\n## [TIME] Training Timeline\n\n```\nTime    Phase                               Duration\n------  ----------------------------------  ---------\n0:00    Cell 1: Setup                       2 min\n0:02    Cell 2: Training starts\n0:02    Load aggregated data                1 min\n0:03    Build Phase 1 features              15 min\n0:18    Build Phase 2-6 features            75 min\n1:33    Train Game: Moneyline               30 min\n2:03    Train Game: Spread                  30 min\n2:33    Train Player: Minutes               60 min\n3:33    Train Player: Points                70 min\n4:43    Train Player: Rebounds              60 min\n5:43    Train Player: Assists               60 min\n6:43    Train Player: Threes                50 min\n7:33    Training complete\n7:33    Cell 3: Validation                  1 min\n7:34    Cell 4: Summary                     10 sec\n7:35    Cell 5: Package + Download          1 min\n------\n7:36    DONE\n```\n\n**Total: ~7.5 hours on P100, ~8.5 hours on T4**\n\n---\n\n## * Quick Start Instructions\n\n### 1. Setup Kaggle Notebook\n- Create new notebook at kaggle.com/code\n- Add \"meeper\" dataset: Click \"Add Data\" -> search \"meeper\" -> Add\n- Enable GPU: Settings -> Accelerator -> GPU P100 or T4\n- Set Internet: On (needed to clone GitHub repo)\n\n### 2. Run Cells\n- **Cell 1** (Setup): 2 minutes\n  - Installs packages\n  - Clones GitHub repo\n  - Verifies dataset exists\n  \n- **Cell 2** (Training): 7-8 hours\n  - This is the main training cell\n  - You can close your browser after starting!\n  - Kaggle keeps running in background\n  \n- **Cell 3** (Validation): 1 minute\n  - Checks that 24-dim embeddings work\n  \n- **Cell 4** (Summary): 10 seconds\n  - Lists all trained models\n  \n- **Cell 5** (Download): 1 minute\n  - Packages models into zip file\n  - Download from Output tab\n\n### 3. Download Trained Models\n- Look at right sidebar -> Output tab\n- Find `nba_models_trained.zip`\n- Click download icon (â†“)\n- Extract to your local `nba_predictor/` folder\n\n---\n\n## * Tips\n\n**Save Money/Time:**\n- Close browser after starting Cell 2 (training continues!)\n- Come back 8 hours later to download models\n- Kaggle gives you 30 hours/week GPU time for free\n\n**Monitor Progress:**\n- Keep notebook tab open to see real-time output\n- Or check back periodically to see which model is training\n\n**If Training Fails:**\n- Check you added \"meeper\" dataset (Add Data menu)\n- Verify GPU is enabled (Settings -> Accelerator)\n- Try running cells 1-2 again (it's safe to restart)\n\n**Expected Output:**\n```\nLoading aggregated data... \n  Loaded 1,632,909 player-games (1947-2026)\n\nBuilding Phase 1 features...\n  Rolling averages (L3, L5, L10)\n  Per-minute rates\n  \nTraining POINTS model...\n  TabNet training (GPU)... 15 min\n  Extracting 24-dim embeddings... 1 min\n  LightGBM training... 2 min\n  MAE: 2.05 (baseline: 2.65) â† 22.6% improvement!\n  [x] Saved: models/points_hybrid_1947_2026.pkl\n```\n\n---\n\n## â“ Troubleshooting\n\n**\"Dataset not found\"**\n- Add \"meeper\" dataset: Add Data -> search \"meeper\" -> Add\n- Restart cell 1\n\n**\"No GPU available\"**\n- Settings -> Accelerator -> GPU (P100 or T4)\n- Restart notebook\n\n**\"Session timeout\"**\n- Kaggle may disconnect after 12 hours (free tier)\n- Models are saved incrementally, won't lose progress\n- Just re-run from where it stopped\n\n**\"Out of memory\"**\n- Shouldn't happen (peak RAM ~2 GB, Kaggle has 13 GB)\n- If it does: Settings -> Restart -> Re-run cells\n\n---\n\n## [DATA] About the Dataset\n\n**Source**: Kaggle dataset \"meeper\" (uploaded by you)\n- **File**: aggregated_nba_data.csv.gzip\n- **Size**: ~100-150 MB compressed\n- **Contents**: Raw NBA box scores + Basketball Reference priors\n- **Date Range**: November 1946 -> November 2025 (80 seasons)\n- **Records**: 1.6 million player-game statistics\n\n**What's Pre-Computed:**\n- [x] Raw box scores (points, rebounds, assists, etc.)\n- [x] Basketball Reference priors (68 advanced stats)\n- [x] Player name fuzzy matching\n\n**What Builds During Training:**\n- Phase 1-6 features (rolling averages, momentum, etc.)\n- This takes ~90 minutes\n\n---\n\n## * Expected Performance\n\n**Game Models:**\n- Moneyline accuracy: 63.5-64.5%\n- Spread RMSE: ~10.2 points\n- Beats Vegas vig (52.4%)\n\n**Player Models:**\n- Points MAE: ~2.0-2.1 (baseline: 2.65)\n- Minutes MAE: ~4.5 (baseline: 6.0)\n- Rebounds MAE: ~1.8 (baseline: 2.3)\n- Assists MAE: ~1.5 (baseline: 2.0)\n- Threes MAE: ~0.9 (baseline: 1.2)\n\n**Why It's Good:**\n- 10x more data than 2002+ cutoff (1.6M vs 125K games)\n- Neural hybrid architecture (TabNet + LightGBM)\n- 24-dimensional embeddings capture player patterns\n- Uncertainty quantification via sigma models\n\n---\n\nReady to start training! Run Cell 1 when you're ready."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}