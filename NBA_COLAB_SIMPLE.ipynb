{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# üèÄ NBA Predictor - Neural Hybrid Training (Game + Player Models)\n\n## Features:\n- ‚úÖ **FULL NBA HISTORY**: 1947-2026 (80 seasons, 1.6M player-games)\n- ‚úÖ **Neural hybrid: TabNet + LightGBM** with 24-dim embeddings\n- ‚úÖ Basketball Reference priors: ~68 advanced stats (already merged!)\n- ‚úÖ **Game Models**: Moneyline + Spread predictions\n- ‚úÖ **Player Models**: Minutes, Points, Rebounds, Assists, Threes\n- ‚úÖ **Phase 1-7 features**: Built automatically during training\n\n## Quick Start:\n1. **Add \"meeper\" dataset** in Kaggle (Add Data ‚Üí search \"meeper\")\n2. **Enable GPU** (P100 or T4 x2)\n3. **Run Cell 1** (setup - 2 min)\n4. **Run Cell 2** (training - 7-8 hours)\n5. **Close browser** - Kaggle keeps running!\n6. **Come back later** and download models\n\n**GPU Required:** P100 (best) or T4 x2\n\n## What You Get:\n- **Game Models** (2): Moneyline classifier + Spread regressor\n- **Player Models** (5): Minutes, Points, Rebounds, Assists, Threes\n- **All with 24-dim embeddings** from TabNet + LightGBM\n- **Uncertainty quantification** via sigma models\n\n## Training Time:\n- **P100**: ~7 hours\n- **T4**: ~8 hours\n- Feature building: 90 min\n- Game models: 1 hour  \n- Player models: 5 hours\n\n## Expected Performance:\n- **Game accuracy**: 63.5-64.5% (beats Vegas 52.4% vig)\n- **Points MAE**: ~2.0-2.1 (22% better than baseline 2.65)\n- **Embeddings**: 24-dimensional, 15-40% feature importance"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ============================================================\n# SETUP (Kaggle Version)\n# ============================================================\n\nprint(\"Installing packages...\")\n!pip install -q pytorch-tabnet lightgbm scikit-learn pandas numpy tqdm\n\nprint(\"\\nDownloading training code from GitHub...\")\nimport os\n\n# Navigate to Kaggle working directory\nos.chdir('/kaggle/working')\n\n# Clone your repository\n!git clone https://github.com/tyriqmiles0529-pixel/meep.git\nos.chdir('meep')\n\nprint(\"\\nCode version:\")\n!git log -1 --oneline\n\n# Check GPU\nimport torch\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Not available'\nprint(f\"\\nGPU: {gpu_name}\")\n\n# Verify dataset exists (added via \"Add Data\" in Kaggle UI)\ndataset_path = '/kaggle/input/meeper/aggregated_nba_data.csv.gzip'\nif os.path.exists(dataset_path):\n    size_mb = os.path.getsize(dataset_path) / 1024 / 1024\n    print(f\"\\nDataset found: {size_mb:.1f} MB\")\n    print(f\"   Path: {dataset_path}\")\n    print(f\"   Full NBA history: 1947-2026 (80 seasons, 1.6M player-games)\")\n    print(f\"   Training will use: ALL DATA (no cutoff)\")\nelse:\n    print(\"\\nDataset not found!\")\n    print(\"   Make sure you added 'meeper' dataset to this notebook\")\n    print(\"   Click 'Add Data' -> search 'meeper' -> Add\")\n\nprint(\"\\nSetup complete!\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ============================================================\n# TRAIN NEURAL HYBRID MODELS - GAME + PLAYER\n# ============================================================\n\nimport os\n\n# Make sure we're in the code directory\nif not os.path.exists('/kaggle/working/meep'):\n    print(\"ERROR: Repository not found!\")\n    print(\"Run Cell 1 first to clone the repository\")\n    raise FileNotFoundError(\"Repository directory /kaggle/working/meep does not exist\")\n\nos.chdir('/kaggle/working/meep')\n\nprint(\"=\"*70)\nprint(\"NBA NEURAL HYBRID TRAINING - GAME + PLAYER MODELS\")\nprint(\"=\"*70)\n\nprint(\"\\nDataset Info:\")\nprint(\"   Source: /kaggle/input/meeper/aggregated_nba_data.csv.gzip\")\nprint(\"   Full range: 1947-2026 (80 seasons, 1.6M player-games)\")\nprint(\"   Training on: ALL DATA (no cutoff)\")\nprint(\"\\nWhat will happen:\")\nprint(\"   1. Load aggregated data (30 sec)\")\nprint(\"   2. Build Phase 1-6 features (90 min)\")\nprint(\"   3. Train game models: Moneyline + Spread (1 hour)\")\nprint(\"   4. Train 5 player props with neural hybrid (5 hours)\")\nprint(\"\\nArchitecture:\")\nprint(\"   Game Models: Ensemble (TabNet + LightGBM)\")\nprint(\"   Player Models: TabNet (24-dim embeddings) + LightGBM\")\n\nimport torch\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n\nif 'P100' in gpu_name:\n    print(\"\\nExpected time: ~7-8 hours total (P100)\")\nelif 'T4' in gpu_name:\n    print(\"\\nExpected time: ~8-9 hours total (T4)\")\nelse:\n    print(\"\\nExpected time: ~7-9 hours\")\n\nprint(\"\\nModels to train:\")\nprint(\"   Game: Moneyline (win probability), Spread (margin)\")\nprint(\"   Player: Minutes, Points, Rebounds, Assists, Threes\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"STARTING TRAINING...\")\nprint(\"=\"*70 + \"\\n\")\n\n# Run training - FULL DATASET, GAME + PLAYER MODELS\n!python train_auto.py \\\n    --aggregated-data /kaggle/input/meeper/aggregated_nba_data.csv.gzip \\\n    --use-neural \\\n    --game-neural \\\n    --neural-epochs 30 \\\n    --neural-device gpu \\\n    --verbose \\\n    --no-window-ensemble\n\n# CORRECT FLAG: --aggregated-data (loads pre-aggregated CSV)\n# NO --skip-game-models = trains both game + player\n# NO --player-season-cutoff = uses all 1947-2026 data\n# --no-window-ensemble = single-pass training (not 5-year windows)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nModels saved to: /kaggle/working/meep/models/\")\nprint(\"\\nNext: Run validation cell to check embeddings\")"
    },
    {
      "cell_type": "code",
      "source": "# ============================================================\n# VALIDATE 24-DIM EMBEDDINGS\n# ============================================================\n\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# Navigate to code directory\nos.chdir('/kaggle/working/meep')\n\nprint(\"üîç Validating TabNet embeddings...\\n\")\n\nmodels_dir = Path('./models')\n\n# Check if models directory exists\nif not models_dir.exists():\n    print(\"‚ùå Models directory not found!\")\n    print(\"   Expected: /kaggle/working/meep/models/\")\n    print(\"   Run training cell first\")\nelse:\n    # Find points model\n    model_files = list(models_dir.glob('*points*.pkl'))\n    \n    if not model_files:\n        print(\"‚ùå No points model found!\")\n        print(f\"   Searching in: {models_dir.absolute()}\")\n        print(f\"   Files found: {list(models_dir.glob('*.pkl'))}\")\n    else:\n        model_path = model_files[0]\n        print(f\"üì¶ Loading model: {model_path.name}\")\n        \n        with open(model_path, 'rb') as f:\n            model = pickle.load(f)\n        \n        print(f\"   Model type: {type(model).__name__}\")\n        \n        # Check if it's a NeuralHybridPredictor\n        if hasattr(model, 'tabnet'):\n            print(f\"   ‚úÖ Neural hybrid detected\")\n            print(f\"   TabNet: {type(model.tabnet).__name__}\")\n            print(f\"   LightGBM: {type(model.lgbm).__name__}\")\n            \n            # Test embedding extraction\n            print(\"\\nüß™ Testing embedding extraction...\")\n            \n            # Create dummy data (match actual feature count)\n            n_features = 150  # Adjust if needed\n            dummy_features = pd.DataFrame(\n                np.random.randn(10, n_features),\n                columns=[f'feature_{i}' for i in range(n_features)]\n            )\n            \n            # Get embeddings\n            try:\n                if hasattr(model, '_get_embeddings'):\n                    embeddings = model._get_embeddings(dummy_features)\n                    print(f\"\\n‚úÖ SUCCESS!\")\n                    print(f\"   Embedding shape: {embeddings.shape}\")\n                    print(f\"   Expected: (10, 24)\")\n                    \n                    if embeddings.shape[1] == 24:\n                        print(f\"\\nüéØ PERFECT: Got 24-dimensional embeddings\")\n                        print(f\"   Mean: {embeddings.mean():.4f}\")\n                        print(f\"   Std: {embeddings.std():.4f}\")\n                        \n                        # Check LightGBM uses embeddings\n                        if hasattr(model.lgbm, 'feature_name_'):\n                            lgbm_features = model.lgbm.feature_name_\n                            embedding_features = [f for f in lgbm_features if 'embedding' in f.lower()]\n                            print(f\"   LightGBM sees {len(embedding_features)} embedding features\")\n                        \n                        print(f\"\\n‚úÖ Model validation PASSED!\")\n                        print(f\"   Ready for predictions\")\n                    else:\n                        print(f\"\\n‚ö†Ô∏è  Warning: Got {embeddings.shape[1]}-dim embeddings\")\n                else:\n                    print(f\"‚ö†Ô∏è  Model doesn't have _get_embeddings method\")\n            \n            except Exception as e:\n                print(f\"‚ùå Error: {e}\")\n                import traceback\n                traceback.print_exc()\n        else:\n            print(f\"   ‚ö†Ô∏è  LightGBM-only model (no neural hybrid)\")\n        \n        # Display model info\n        print(f\"\\nüìä Model Summary:\")\n        if hasattr(model, 'lgbm'):\n            print(f\"   LightGBM trees: {model.lgbm.n_estimators if hasattr(model.lgbm, 'n_estimators') else 'N/A'}\")\n            if hasattr(model.lgbm, 'feature_name_'):\n                print(f\"   Features used: {len(model.lgbm.feature_name_)}\")\n        \n        if hasattr(model, 'sigma_model'):\n            print(f\"   Uncertainty model: {'Yes' if model.sigma_model else 'No'}\")\n\nprint(\"\\n‚úÖ Validation complete!\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ============================================================\n# TRAINING RESULTS SUMMARY\n# ============================================================\n\nimport os\nfrom pathlib import Path\n\nos.chdir('/kaggle/working/meep')\n\nmodels_dir = Path('./models')\n\nprint(\"=\"*70)\nprint(\"TRAINING RESULTS\")\nprint(\"=\"*70)\n\nif not models_dir.exists():\n    print(\"\nNo models directory found!\")\nelse:\n    model_files = list(models_dir.glob('*.pkl'))\n    \n    if not model_files:\n        print(\"\nNo models found!\")\n        print(f\"   Directory: {models_dir.absolute()}\")\n    else:\n        print(f\"\nFound {len(model_files)} trained models:\n\")\n        \n        for model_path in sorted(model_files):\n            print(f\"   {model_path.name}\")\n            size_mb = model_path.stat().st_size / 1024 / 1024\n            print(f\"      Size: {size_mb:.1f} MB\")\n            \n            # Try to load and check type\n            try:\n                import pickle\n                with open(model_path, 'rb') as f:\n                    model = pickle.load(f)\n                \n                if hasattr(model, 'tabnet'):\n                    print(f\"      Type: Neural Hybrid\")\n                else:\n                    print(f\"      Type: LightGBM only\")\n                \n                if hasattr(model, 'sigma_model') and model.sigma_model:\n                    print(f\"      Uncertainty: Yes\")\n                \n                print()\n            except Exception as e:\n                print(f\"      Error loading: {e}\n\")\n\nprint(\"=\"*70)\nprint(\"\nModels location: /kaggle/working/meep/models/\")\nprint(\"\nNext: Package and download models\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ============================================================\n# PACKAGE MODELS FOR DOWNLOAD\n# ============================================================\n\nimport os\n\nos.chdir('/kaggle/working')\n\nprint(\"üì¶ Packaging models...\")\n\n# Check if models exist\nif not os.path.exists('meep/models'):\n    print(\"\\n‚ùå No models directory found!\")\n    print(\"   Run training first\")\nelse:\n    # Create zip file\n    !zip -r nba_models_trained.zip meep/models/ meep/model_cache/ 2>/dev/null\n    \n    # Check if zip was created\n    if os.path.exists('nba_models_trained.zip'):\n        size_mb = os.path.getsize('nba_models_trained.zip') / 1024 / 1024\n        print(f\"\\n‚úÖ Package created: nba_models_trained.zip ({size_mb:.1f} MB)\")\n        print(\"\\nüì• To download:\")\n        print(\"   1. Look at the right sidebar\")\n        print(\"   2. Click 'Output' tab\")\n        print(\"   3. Find 'nba_models_trained.zip'\")\n        print(\"   4. Click the download icon (‚Üì)\")\n        print(\"\\nOr use this command to download via Kaggle API:\")\n        print(\"   (This requires Kaggle notebook to be public)\")\n    else:\n        print(\"\\n‚ùå Failed to create zip file\")\n        print(\"   Check if models exist in meep/models/\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## üìã What You Get After Training\n\n### Game Models (2 files):\n- `moneyline_ensemble_1947_2026.pkl` - Win probability predictions\n- `spread_ensemble_1947_2026.pkl` - Margin predictions\n- Both with TabNet + LightGBM ensemble architecture\n- Expected accuracy: 63.5-64.5%\n\n### Player Models (5 files):\n- `minutes_hybrid_1947_2026.pkl`\n- `points_hybrid_1947_2026.pkl`\n- `rebounds_hybrid_1947_2026.pkl`\n- `assists_hybrid_1947_2026.pkl`\n- `threes_hybrid_1947_2026.pkl`\n- All with 24-dimensional TabNet embeddings + LightGBM\n- Points MAE: ~2.0-2.1 (22% improvement over baseline)\n\n### Features Included:\n- **Raw stats**: 40 box score features from 1947-2026\n- **Basketball Reference priors**: 68 advanced stats (already merged!)\n- **Phase 1-6 features**: 150+ engineered features (built during training)\n- **Total**: ~235 features per prediction\n\n---\n\n## ‚è±Ô∏è Training Timeline\n\n```\nTime    Phase                               Duration\n------  ----------------------------------  ---------\n0:00    Cell 1: Setup                       2 min\n0:02    Cell 2: Training starts\n0:02    Load aggregated data                1 min\n0:03    Build Phase 1 features              15 min\n0:18    Build Phase 2-6 features            75 min\n1:33    Train Game: Moneyline               30 min\n2:03    Train Game: Spread                  30 min\n2:33    Train Player: Minutes               60 min\n3:33    Train Player: Points                70 min\n4:43    Train Player: Rebounds              60 min\n5:43    Train Player: Assists               60 min\n6:43    Train Player: Threes                50 min\n7:33    Training complete\n7:33    Cell 3: Validation                  1 min\n7:34    Cell 4: Summary                     10 sec\n7:35    Cell 5: Package + Download          1 min\n------\n7:36    DONE\n```\n\n**Total: ~7.5 hours on P100, ~8.5 hours on T4**\n\n---\n\n## üöÄ Quick Start Instructions\n\n### 1. Setup Kaggle Notebook\n- Create new notebook at kaggle.com/code\n- Add \"meeper\" dataset: Click \"Add Data\" ‚Üí search \"meeper\" ‚Üí Add\n- Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU P100 or T4\n- Set Internet: On (needed to clone GitHub repo)\n\n### 2. Run Cells\n- **Cell 1** (Setup): 2 minutes\n  - Installs packages\n  - Clones GitHub repo\n  - Verifies dataset exists\n  \n- **Cell 2** (Training): 7-8 hours\n  - This is the main training cell\n  - You can close your browser after starting!\n  - Kaggle keeps running in background\n  \n- **Cell 3** (Validation): 1 minute\n  - Checks that 24-dim embeddings work\n  \n- **Cell 4** (Summary): 10 seconds\n  - Lists all trained models\n  \n- **Cell 5** (Download): 1 minute\n  - Packages models into zip file\n  - Download from Output tab\n\n### 3. Download Trained Models\n- Look at right sidebar ‚Üí Output tab\n- Find `nba_models_trained.zip`\n- Click download icon (‚Üì)\n- Extract to your local `nba_predictor/` folder\n\n---\n\n## üí° Tips\n\n**Save Money/Time:**\n- Close browser after starting Cell 2 (training continues!)\n- Come back 8 hours later to download models\n- Kaggle gives you 30 hours/week GPU time for free\n\n**Monitor Progress:**\n- Keep notebook tab open to see real-time output\n- Or check back periodically to see which model is training\n\n**If Training Fails:**\n- Check you added \"meeper\" dataset (Add Data menu)\n- Verify GPU is enabled (Settings ‚Üí Accelerator)\n- Try running cells 1-2 again (it's safe to restart)\n\n**Expected Output:**\n```\nLoading aggregated data... \n  Loaded 1,632,909 player-games (1947-2026)\n\nBuilding Phase 1 features...\n  Rolling averages (L3, L5, L10)\n  Per-minute rates\n  \nTraining POINTS model...\n  TabNet training (GPU)... 15 min\n  Extracting 24-dim embeddings... 1 min\n  LightGBM training... 2 min\n  MAE: 2.05 (baseline: 2.65) ‚Üê 22.6% improvement!\n  ‚úÖ Saved: models/points_hybrid_1947_2026.pkl\n```\n\n---\n\n## ‚ùì Troubleshooting\n\n**\"Dataset not found\"**\n- Add \"meeper\" dataset: Add Data ‚Üí search \"meeper\" ‚Üí Add\n- Restart cell 1\n\n**\"No GPU available\"**\n- Settings ‚Üí Accelerator ‚Üí GPU (P100 or T4)\n- Restart notebook\n\n**\"Session timeout\"**\n- Kaggle may disconnect after 12 hours (free tier)\n- Models are saved incrementally, won't lose progress\n- Just re-run from where it stopped\n\n**\"Out of memory\"**\n- Shouldn't happen (peak RAM ~2 GB, Kaggle has 13 GB)\n- If it does: Settings ‚Üí Restart ‚Üí Re-run cells\n\n---\n\n## üìä About the Dataset\n\n**Source**: Kaggle dataset \"meeper\" (uploaded by you)\n- **File**: aggregated_nba_data.csv.gzip\n- **Size**: ~100-150 MB compressed\n- **Contents**: Raw NBA box scores + Basketball Reference priors\n- **Date Range**: November 1946 ‚Üí November 2025 (80 seasons)\n- **Records**: 1.6 million player-game statistics\n\n**What's Pre-Computed:**\n- ‚úÖ Raw box scores (points, rebounds, assists, etc.)\n- ‚úÖ Basketball Reference priors (68 advanced stats)\n- ‚úÖ Player name fuzzy matching\n\n**What Builds During Training:**\n- Phase 1-6 features (rolling averages, momentum, etc.)\n- This takes ~90 minutes\n\n---\n\n## üéØ Expected Performance\n\n**Game Models:**\n- Moneyline accuracy: 63.5-64.5%\n- Spread RMSE: ~10.2 points\n- Beats Vegas vig (52.4%)\n\n**Player Models:**\n- Points MAE: ~2.0-2.1 (baseline: 2.65)\n- Minutes MAE: ~4.5 (baseline: 6.0)\n- Rebounds MAE: ~1.8 (baseline: 2.3)\n- Assists MAE: ~1.5 (baseline: 2.0)\n- Threes MAE: ~0.9 (baseline: 1.2)\n\n**Why It's Good:**\n- 10x more data than 2002+ cutoff (1.6M vs 125K games)\n- Neural hybrid architecture (TabNet + LightGBM)\n- 24-dimensional embeddings capture player patterns\n- Uncertainty quantification via sigma models\n\n---\n\nReady to start training! Run Cell 1 when you're ready."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}