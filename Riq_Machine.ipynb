{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸŽ¯ RIQ MACHINE - NBA Prediction & Analysis\n\n## Features:\n- âœ… Load trained neural hybrid models (TabNet + LightGBM)\n- âœ… Fetch live NBA games and player stats\n- âœ… Generate predictions with full feature engineering\n- âœ… Compare to betting lines (find value)\n- âœ… Backtest on historical data\n- âœ… Track performance and ROI\n\n## Prerequisites:\n1. Trained models from `NBA_COLAB_SIMPLE.ipynb`\n2. Three separate files to upload:\n   - `nba_models_trained.zip` (trained models)\n   - `priors_data.zip` (Basketball Reference priors)\n   - `PlayerStatistics.csv` (historical data for backtesting)\n\n## Quick Start:\n1. Run Setup cell\n2. Upload 3 files (separate prompts)\n3. Choose: Backtest OR Live Predictions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¦ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Install packages and download code\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ“¦ Installing packages...\")\n",
    "!pip install -q nba-api pytorch-tabnet lightgbm scikit-learn pandas numpy requests\n",
    "\n",
    "print(\"\\nðŸ“¥ Downloading training code (for feature engineering)...\")\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "os.chdir('/content')\n",
    "\n",
    "# Remove old code if exists\n",
    "if os.path.exists('meep'):\n",
    "    shutil.rmtree('meep')\n",
    "    print(\"ðŸ§¹ Cleaned up old code\")\n",
    "\n",
    "!git clone https://github.com/tyriqmiles0529-pixel/meep.git\n",
    "os.chdir('meep')\n",
    "\n",
    "print(\"\\nðŸ“ Code version:\")\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Add to Python path so we can import train_auto\n",
    "import sys\n",
    "sys.path.insert(0, '/content/meep')\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\")\n",
    "print(\"\\nNext: Run 'Upload Models & Data' cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¤ Upload Models & Data"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# UPLOAD - Models, priors data, and player statistics\n# ============================================================\n\nfrom google.colab import files\nimport os\nimport zipfile\n\nos.chdir('/content')\n\nprint(\"=\"*70)\nprint(\"ðŸ“¤ UPLOAD ALL FILES\")\nprint(\"=\"*70)\nprint(\"\\nUpload ALL 3 files at once:\")\nprint(\"  1. nba_models_trained.zip (your trained models)\")\nprint(\"  2. priors_data.zip (Basketball Reference priors)\")\nprint(\"  3. PlayerStatistics.csv.zip (historical data)\")\nprint(\"\\nUploading...\\n\")\n\nuploaded = files.upload()\n\n# Extract models\nif os.path.exists('nba_models_trained.zip'):\n    print(\"\\nðŸ“¦ Extracting models...\")\n    !unzip -q nba_models_trained.zip\n    !rm nba_models_trained.zip\n    print(\"âœ… Models extracted to ./models/ and ./model_cache/\")\nelse:\n    print(\"âš ï¸  nba_models_trained.zip not found\")\n\n# Extract priors\nif os.path.exists('priors_data.zip'):\n    print(\"\\nðŸ“¦ Extracting priors...\")\n    !unzip -q priors_data.zip\n    !rm priors_data.zip\n    print(\"âœ… Priors extracted to ./priors_data/\")\nelse:\n    print(\"âš ï¸  priors_data.zip not found\")\n\n# Extract player stats\nif os.path.exists('PlayerStatistics.csv.zip'):\n    print(\"\\nðŸ“¦ Extracting PlayerStatistics.csv...\")\n    !unzip -q PlayerStatistics.csv.zip\n    !rm PlayerStatistics.csv.zip\n    if os.path.exists('PlayerStatistics.csv'):\n        size_mb = os.path.getsize('PlayerStatistics.csv') / 1024 / 1024\n        print(f\"âœ… PlayerStatistics.csv extracted ({size_mb:.1f} MB)\")\n    else:\n        print(\"âŒ Extraction failed\")\nelse:\n    print(\"âš ï¸  PlayerStatistics.csv.zip not found\")\n\n# Final verification\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸ” FINAL VERIFICATION\")\nprint(\"=\"*70)\n\nif os.path.exists('models'):\n    models = [f for f in os.listdir('models') if f.endswith('.pkl')]\n    print(f\"âœ… Found {len(models)} model files\")\nelse:\n    print(\"âŒ models/ directory not found\")\n\nif os.path.exists('priors_data'):\n    priors = [f for f in os.listdir('priors_data') if f.endswith('.csv')]\n    print(f\"âœ… Found {len(priors)} priors files\")\nelse:\n    print(\"âŒ priors_data/ directory not found\")\n\nif os.path.exists('PlayerStatistics.csv'):\n    size_mb = os.path.getsize('PlayerStatistics.csv') / 1024 / 1024\n    print(f\"âœ… PlayerStatistics.csv ready ({size_mb:.1f} MB)\")\nelse:\n    print(\"âŒ PlayerStatistics.csv not found\")\n\nprint(\"\\nâœ… Upload complete!\")\nprint(\"\\nNext: Configure API keys and run analysis\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ðŸ”‘ Configure API Keys",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# API KEYS - Required for betting odds\n# ============================================================\n\nimport os\n\n# Your API keys (from keys.py)\nAPI_SPORTS_KEY = \"4979ac5e1f7ae10b1d6b58f1bba01140\"\nRAPIDAPI_KEY = \"9ef7289093msh76adf5ee5bedb5fp15e0d6jsnc2a0d0ed9abe\"\nTHEODDS_API_KEY = \"c98703301e8f89ef2c3648a4373939fd\"\n\n# Set environment variables\nos.environ[\"API_SPORTS_KEY\"] = API_SPORTS_KEY\nos.environ[\"APISPORTS_KEY\"] = API_SPORTS_KEY\nos.environ[\"RAPIDAPI_KEY\"] = RAPIDAPI_KEY\nos.environ[\"THEODDS_API_KEY\"] = THEODDS_API_KEY\n\nprint(\"=\"*70)\nprint(\"ðŸ”‘ API KEY STATUS\")\nprint(\"=\"*70)\nprint(f\"API-Sports: âœ… Set ({API_SPORTS_KEY[:8]}...)\")\nprint(f\"RapidAPI:   âœ… Set ({RAPIDAPI_KEY[:8]}...)\")\nprint(f\"The Odds:   âœ… Set ({THEODDS_API_KEY[:8]}...)\")\nprint(\"\\nâœ… All API keys configured!\")\nprint(\"\\nNext: Run analysis cell to fetch games and generate predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ============================================================\n# RUN FULL ANALYSIS - Uses riq_analyzer.py\n# ============================================================\n\nimport sys\nsys.path.insert(0, '/content/meep')\n\n# Import and run riq_analyzer main function\nprint(\"=\"*72)\nprint(\"ðŸŽ¯ RUNNING FULL NBA PREDICTION ANALYSIS\")\nprint(\"=\"*72)\nprint(\"\\nThis uses riq_analyzer.py to:\")\nprint(\"  1. Fetch upcoming NBA games (API-Sports)\")\nprint(\"  2. Fetch betting odds (TheRundown/RapidAPI)\")\nprint(\"  3. Load trained models\")\nprint(\"  4. Generate predictions\")\nprint(\"  5. Compare predictions vs lines\")\nprint(\"  6. Calculate EV, Kelly, ELG scores\")\nprint(\"  7. Build optimal parlays\")\nprint(\"\\n\" + \"=\"*72 + \"\\n\")\n\ntry:\n    # Import riq_analyzer and run main\n    import riq_analyzer\n    \n    # Run main analysis\n    riq_analyzer.main()\n    \nexcept ImportError as e:\n    print(f\"âŒ Failed to import riq_analyzer: {e}\")\n    print(\"\\nâš ï¸  Make sure you ran the Setup cell to clone the repo\")\n    \nexcept Exception as e:\n    print(f\"âŒ Analysis failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    print(\"\\nðŸ’¡ Possible issues:\")\n    print(\"  - API keys not set (run API Keys cell)\")\n    print(\"  - No games today\")\n    print(\"  - API rate limits exceeded\")\n    print(\"  - Models not uploaded\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BACKTEST - Test models on recent historical games\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ§ª BACKTEST MODE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load trained models\n",
    "print(\"\\nðŸ“Š Loading models...\")\n",
    "models = {}\n",
    "props = ['minutes', 'points', 'rebounds', 'assists', 'threes']\n",
    "\n",
    "for prop in props:\n",
    "    model_path = Path(f'models/{prop}_model.pkl')\n",
    "    if model_path.exists():\n",
    "        with open(model_path, 'rb') as f:\n",
    "            models[prop] = pickle.load(f)\n",
    "        print(f\"   âœ… {prop.capitalize()}: {len(models[prop].feature_names)} features\")\n",
    "    else:\n",
    "        print(f\"   âŒ {prop.capitalize()}: Not found\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(models)} models\")\n",
    "\n",
    "# Check model structure\n",
    "print(\"\\nðŸ” Model Structure (Points):\")\n",
    "points_model = models.get('points')\n",
    "if points_model:\n",
    "    print(f\"   TabNet: {'âœ…' if hasattr(points_model, 'tabnet') else 'âŒ'}\")\n",
    "    print(f\"   LightGBM: {'âœ…' if hasattr(points_model, 'lgbm') else 'âŒ'}\")\n",
    "    print(f\"   Embedding scaler: {'âœ…' if hasattr(points_model, 'embedding_scaler') else 'âŒ'}\")\n",
    "    print(f\"   Total features: {len(points_model.feature_names)}\")\n",
    "    \n",
    "    # Show feature importance\n",
    "    if hasattr(points_model, 'lgbm') and hasattr(points_model.lgbm, 'feature_importances_'):\n",
    "        importances = sorted(\n",
    "            zip(points_model.feature_names + [f'tabnet_emb_{i}' for i in range(24)], \n",
    "                points_model.lgbm.feature_importances_),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:10]\n",
    "        \n",
    "        print(\"\\nðŸ§  Top 10 Most Important Features:\")\n",
    "        for feat, imp in importances:\n",
    "            emoji = \"ðŸ§ \" if 'tabnet_emb' in feat else \"ðŸ“Š\"\n",
    "            print(f\"   {emoji} {feat}: {imp:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Œ TODO: Add backtest logic\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Load validation data from training\")\n",
    "print(\"2. Generate predictions\")\n",
    "print(\"3. Compare to actual results\")\n",
    "print(\"4. Calculate RMSE, MAE, directional accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE ENGINEERING - Reuse training code\n",
    "# ============================================================\n",
    "\n",
    "# Import feature engineering from train_auto.py\n",
    "# This ensures features match training exactly!\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/meep')\n",
    "\n",
    "try:\n",
    "    from train_auto import build_players_from_playerstats\n",
    "    print(\"âœ… Imported feature engineering from train_auto.py\")\n",
    "    print(\"   This ensures features match training schema exactly!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import: {e}\")\n",
    "    print(\"   Make sure you ran the Setup cell first\")\n",
    "\n",
    "# TODO: Add helper functions for:\n",
    "# - Fetching player recent games\n",
    "# - Converting API data to training schema\n",
    "# - Generating predictions\n",
    "# - Comparing to betting lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BETTING ANALYSIS - Find value bets\n",
    "# ============================================================\n",
    "\n",
    "def find_value_bets(predictions, lines, threshold=2.0):\n",
    "    \"\"\"\n",
    "    Find value bets where model disagrees with betting line.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dict of player predictions {player_name: {prop: prediction}}\n",
    "        lines: Dict of betting lines {player_name: {prop: line}}\n",
    "        threshold: Minimum difference to consider (default: 2.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of value bets sorted by edge\n",
    "    \"\"\"\n",
    "    value_bets = []\n",
    "    \n",
    "    for player, preds in predictions.items():\n",
    "        if player not in lines:\n",
    "            continue\n",
    "        \n",
    "        player_lines = lines[player]\n",
    "        \n",
    "        for prop, pred_value in preds.items():\n",
    "            if prop not in player_lines:\n",
    "                continue\n",
    "            \n",
    "            line = player_lines[prop]\n",
    "            edge = abs(pred_value - line)\n",
    "            \n",
    "            if edge >= threshold:\n",
    "                direction = 'OVER' if pred_value > line else 'UNDER'\n",
    "                \n",
    "                value_bets.append({\n",
    "                    'player': player,\n",
    "                    'prop': prop,\n",
    "                    'prediction': pred_value,\n",
    "                    'line': line,\n",
    "                    'edge': edge,\n",
    "                    'direction': direction,\n",
    "                    'confidence': edge / threshold  # Simple confidence metric\n",
    "                })\n",
    "    \n",
    "    # Sort by edge (highest first)\n",
    "    value_bets.sort(key=lambda x: x['edge'], reverse=True)\n",
    "    \n",
    "    return value_bets\n",
    "\n",
    "# TODO: Add Kelly criterion bet sizing\n",
    "# TODO: Add odds fetching (API integration)\n",
    "# TODO: Add historical bet tracking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}