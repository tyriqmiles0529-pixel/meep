{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Convert NBA CSV to Parquet\n",
    "\n",
    "**Steps:**\n",
    "1. Add \"meeper\" dataset (Add Data -> search \"meeper\")\n",
    "2. Run the cell below (~10 min)\n",
    "3. Download parquet from Output tab\n",
    "4. Upload as new Kaggle dataset\n",
    "\n",
    "**No GPU needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nimport gc\n\ncsv_path = '/kaggle/input/meeper/aggregated_nba_data.csv/aggregated_nba_data.csv.gzip'\nparquet_path = 'aggregated_nba_data.parquet'\n\nprint(\"=\"*60)\nprint(\"CHUNKED CSV TO PARQUET CONVERSION (PyArrow)\")\nprint(\"=\"*60)\nprint(\"Loading in chunks to avoid memory crashes...\\n\")\n\n# Read and write in chunks using PyArrow (no data loss!)\nchunk_size = 500000  # 500K rows at a time\ntotal_rows = 0\nwriter = None\n\nfor i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size, low_memory=False)):\n    total_rows += len(chunk)\n    print(f\"Chunk {i+1}: {total_rows:,} rows processed...\")\n    \n    # Convert to PyArrow table\n    table = pa.Table.from_pandas(chunk, preserve_index=False)\n    \n    if writer is None:\n        # First chunk: create writer with schema\n        writer = pq.ParquetWriter(parquet_path, table.schema, compression='snappy')\n    \n    # Write chunk to parquet\n    writer.write_table(table)\n    \n    del chunk, table\n    gc.collect()\n\n# Close writer to finalize file\nif writer:\n    writer.close()\n\nprint(f\"\\nTotal rows written: {total_rows:,}\")\n\n# Verify the parquet file\npf = pq.ParquetFile(parquet_path)\nactual_rows = pf.metadata.num_rows\nprint(f\"Parquet rows (verified): {actual_rows:,}\")\n\nif actual_rows != total_rows:\n    print(f\"WARNING: Row count mismatch! Expected {total_rows:,}, got {actual_rows:,}\")\nelse:\n    print(\"Row count verified OK!\")\n\nsize_mb = os.path.getsize(parquet_path) / 1024**2\nprint(f\"\\nParquet file: {size_mb:.1f} MB\")\n\n# Show year range\nsample = pd.read_parquet(parquet_path, columns=['season', 'game_year'], engine='pyarrow')\nprint(f\"Season range: {sample['season'].min()} - {sample['season'].max()}\")\nprint(f\"Game year range: {sample['game_year'].min()} - {sample['game_year'].max()}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DONE! Download from Output tab\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}