{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Convert NBA CSV to Parquet\n",
    "\n",
    "**Steps:**\n",
    "1. Add \"meeper\" dataset (Add Data -> search \"meeper\")\n",
    "2. Run the cell below (~10 min)\n",
    "3. Download parquet from Output tab\n",
    "4. Upload as new Kaggle dataset\n",
    "\n",
    "**No GPU needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nimport gc\n\ncsv_path = '/kaggle/input/meeper/aggregated_nba_data.csv/aggregated_nba_data.csv.gzip'\nparquet_path = 'aggregated_nba_data.parquet'\n\nprint(\"=\"*60)\nprint(\"CHUNKED CSV TO PARQUET CONVERSION (PyArrow)\")\nprint(\"=\"*60)\nprint(\"Loading in chunks to avoid memory crashes...\\n\")\n\n# First pass: read first chunk to get schema, then standardize ALL dtypes\nprint(\"Step 1: Reading first chunk to establish schema...\")\nfirst_chunk = pd.read_csv(csv_path, nrows=1000, low_memory=False)\n\n# Create dtype mapping - convert ALL object columns to string, numeric to float\ndtype_mapping = {}\nfor col in first_chunk.columns:\n    if first_chunk[col].dtype == 'object':\n        dtype_mapping[col] = str\n    elif first_chunk[col].dtype in ['int64', 'float64']:\n        # Keep as-is but will handle NaN conversion\n        pass\n\nprint(f\"Found {len(first_chunk.columns)} columns\")\ndel first_chunk\ngc.collect()\n\n# Read and write in chunks using PyArrow (no data loss!)\nprint(\"\\nStep 2: Converting chunks to Parquet...\")\nchunk_size = 500000  # 500K rows at a time\ntotal_rows = 0\nwriter = None\nschema = None\n\nfor i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size, low_memory=False, dtype=dtype_mapping)):\n    total_rows += len(chunk)\n    print(f\"Chunk {i+1}: {total_rows:,} rows processed...\")\n    \n    # Standardize dtypes to prevent schema mismatch\n    # Convert all object columns to string explicitly\n    for col in chunk.select_dtypes(include=['object']).columns:\n        chunk[col] = chunk[col].astype(str)\n    \n    # Convert to PyArrow table\n    table = pa.Table.from_pandas(chunk, preserve_index=False)\n    \n    if writer is None:\n        # First chunk: create writer with schema\n        schema = table.schema\n        writer = pq.ParquetWriter(parquet_path, schema, compression='snappy')\n        print(f\"Schema established with {len(schema)} columns\")\n    else:\n        # Ensure schema matches by casting if needed\n        try:\n            table = table.cast(schema)\n        except Exception as e:\n            print(f\"Warning: Schema cast issue in chunk {i+1}, attempting column-by-column fix...\")\n            # Manual fix: rebuild table with consistent types\n            arrays = []\n            for field in schema:\n                if field.name in chunk.columns:\n                    col_data = chunk[field.name]\n                    # Force convert to expected type\n                    if pa.types.is_string(field.type):\n                        col_data = col_data.astype(str)\n                    arrays.append(pa.array(col_data, type=field.type, from_pandas=True))\n                else:\n                    # Missing column - fill with nulls\n                    arrays.append(pa.nulls(len(chunk), type=field.type))\n            table = pa.Table.from_arrays(arrays, schema=schema)\n    \n    # Write chunk to parquet\n    writer.write_table(table)\n    \n    del chunk, table\n    gc.collect()\n\n# Close writer to finalize file\nif writer:\n    writer.close()\n\nprint(f\"\\nTotal rows written: {total_rows:,}\")\n\n# Verify the parquet file\npf = pq.ParquetFile(parquet_path)\nactual_rows = pf.metadata.num_rows\nprint(f\"Parquet rows (verified): {actual_rows:,}\")\n\nif actual_rows != total_rows:\n    print(f\"WARNING: Row count mismatch! Expected {total_rows:,}, got {actual_rows:,}\")\nelse:\n    print(\"Row count verified OK!\")\n\nsize_mb = os.path.getsize(parquet_path) / 1024**2\nprint(f\"\\nParquet file: {size_mb:.1f} MB\")\n\n# Show year range\nprint(\"\\nVerifying year range...\")\nsample = next(pq.ParquetFile(parquet_path).iter_batches(columns=['season', 'game_year'], batch_size=100000))\nsample_df = sample.to_pandas()\nprint(f\"Sample season range: {sample_df['season'].min()} - {sample_df['season'].max()}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DONE! Download from Output tab\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}