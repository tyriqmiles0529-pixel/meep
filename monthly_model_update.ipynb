{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Monthly NBA Model Update - Incremental Training\n",
    "\n",
    "**Purpose:** Update your NBA prediction models monthly with new games WITHOUT full retrain\n",
    "\n",
    "**Strategy:**\n",
    "- Full historical data: 1947-present (team models)\n",
    "- Player models: 2022-present (Kaggle dataset limitation)\n",
    "- LightGBM: Warm start from previous month\n",
    "- TabNet: Retrain on sliding window (last 3 years)\n",
    "- Recalibration: Update weekly\n",
    "\n",
    "**Runtime:** ~30-60 minutes on GPU (vs 2-3 hours full retrain)\n",
    "\n",
    "---\n",
    "\n",
    "## When to Run:\n",
    "- **Monthly:** Full update (this notebook)\n",
    "- **Weekly:** Recalibration only (faster, separate script)\n",
    "- **Yearly:** Full retrain recommended (quality check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Step 1: Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install -q pytorch-tabnet lightgbm scikit-learn pandas numpy torch kaggle\n",
    "print(\"[OK] Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu"
   },
   "source": [
    "## Step 2: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[OK] GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    print(\"[WARNING] No GPU - will be slower\")\n",
    "    USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "## Step 3: Clone Your GitHub Repo\n",
    "\n",
    "**Option A:** Clone from GitHub (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone your repo (replace with your actual repo URL)\n",
    "!git clone https://github.com/YOUR_USERNAME/nba_predictor.git\n",
    "%cd nba_predictor\n",
    "\n",
    "print(\"[OK] Repo cloned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_alt"
   },
   "source": [
    "**Option B:** Upload files manually (if not using GitHub)\n",
    "\n",
    "Upload these files:\n",
    "- `train_auto.py`\n",
    "- `neural_hybrid.py`\n",
    "- `monthly_update.py` (we'll create this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaggle"
   },
   "source": [
    "## Step 4: Setup Kaggle Credentials\n",
    "\n",
    "Upload your `kaggle.json` file or set credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaggle_setup"
   },
   "outputs": [],
   "source": [
    "# Option A: Upload kaggle.json\n",
    "from google.colab import files\n",
    "files.upload()  # Upload kaggle.json\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"[OK] Kaggle credentials configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_data"
   },
   "source": [
    "## Step 5: Download Latest Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version of dataset\n",
    "print(\"[INFO] Downloading latest NBA data from Kaggle...\")\n",
    "path = kagglehub.dataset_download(\"eoinamoore/historical-nba-data-and-player-box-scores\")\n",
    "\n",
    "print(f\"[OK] Data downloaded to: {path}\")\n",
    "\n",
    "# Check what's new\n",
    "import pandas as pd\n",
    "games_df = pd.read_csv(f\"{path}/Game.csv\")\n",
    "games_df['GAME_DATE_EST'] = pd.to_datetime(games_df['GAME_DATE_EST'])\n",
    "\n",
    "latest_game = games_df['GAME_DATE_EST'].max()\n",
    "print(f\"\\n[INFO] Latest game in dataset: {latest_game}\")\n",
    "print(f\"[INFO] Total games: {len(games_df):,}\")\n",
    "print(f\"[INFO] Date range: {games_df['GAME_DATE_EST'].min()} to {latest_game}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategy"
   },
   "source": [
    "## Step 6: Configure Update Strategy\n",
    "\n",
    "Choose your training approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Update configuration\n",
    "UPDATE_CONFIG = {\n",
    "    # Data configuration\n",
    "    'full_history': True,          # Use all data back to 1947 (team models)\n",
    "    'player_start_year': 2022,     # Player data only available from 2022\n",
    "    \n",
    "    # Training strategy\n",
    "    'strategy': 'hybrid',          # 'full_retrain', 'sliding_window', or 'hybrid'\n",
    "    'sliding_window_years': 3,     # For sliding window: last N years\n",
    "    \n",
    "    # LightGBM warm start\n",
    "    'lgb_warm_start': True,        # Build on previous model\n",
    "    'lgb_new_rounds': 200,         # Additional boosting rounds\n",
    "    \n",
    "    # TabNet settings\n",
    "    'tabnet_retrain': 'monthly',   # 'monthly' or 'quarterly'\n",
    "    'tabnet_epochs': 30,           # Epochs for TabNet (reduce for faster)\n",
    "    \n",
    "    # Model versioning\n",
    "    'version': datetime.datetime.now().strftime('%Y%m'),  # e.g., '202411'\n",
    "    'previous_version': None,      # Set to previous month's version for warm start\n",
    "}\n",
    "\n",
    "print(\"[CONFIG] Update Strategy:\")\n",
    "print(f\"  Strategy: {UPDATE_CONFIG['strategy']}\")\n",
    "print(f\"  LightGBM warm start: {UPDATE_CONFIG['lgb_warm_start']}\")\n",
    "print(f\"  TabNet retrain: {UPDATE_CONFIG['tabnet_retrain']}\")\n",
    "print(f\"  Model version: v{UPDATE_CONFIG['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "identify_new"
   },
   "source": [
    "## Step 7: Identify New Games Since Last Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "new_games"
   },
   "outputs": [],
   "source": [
    "# Set the date of your last model update\n",
    "# Change this to the date you last trained\n",
    "LAST_UPDATE_DATE = '2024-10-01'  # CHANGE THIS\n",
    "\n",
    "last_update = pd.to_datetime(LAST_UPDATE_DATE)\n",
    "new_games = games_df[games_df['GAME_DATE_EST'] > last_update]\n",
    "\n",
    "print(f\"\\n[INFO] Games since last update ({LAST_UPDATE_DATE}):\")\n",
    "print(f\"  New games: {len(new_games):,}\")\n",
    "print(f\"  Date range: {new_games['GAME_DATE_EST'].min()} to {new_games['GAME_DATE_EST'].max()}\")\n",
    "\n",
    "if len(new_games) == 0:\n",
    "    print(\"\\n[WARNING] No new games found. Your model is already up to date!\")\n",
    "else:\n",
    "    print(f\"\\n[OK] Found {len(new_games)} new games to incorporate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_previous"
   },
   "source": [
    "## Step 8: Load Previous Models (for Warm Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_models"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Check if previous models exist\n",
    "PREVIOUS_MODEL_DIR = 'models_cache'  # Your model directory\n",
    "\n",
    "if UPDATE_CONFIG['lgb_warm_start'] and UPDATE_CONFIG['previous_version']:\n",
    "    prev_version = UPDATE_CONFIG['previous_version']\n",
    "    prev_model_path = f\"{PREVIOUS_MODEL_DIR}/ensemble_{prev_version}.pkl\"\n",
    "    \n",
    "    if os.path.exists(prev_model_path):\n",
    "        print(f\"[OK] Found previous model: {prev_model_path}\")\n",
    "        with open(prev_model_path, 'rb') as f:\n",
    "            previous_models = pickle.load(f)\n",
    "        print(f\"[OK] Loaded previous models for warm start\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Previous model not found, will do full retrain\")\n",
    "        UPDATE_CONFIG['lgb_warm_start'] = False\n",
    "else:\n",
    "    print(\"[INFO] No warm start - will train from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## Step 9: Run Incremental Training\n",
    "\n",
    "**Choose your approach:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "approach_a"
   },
   "source": [
    "### Approach A: Full Retrain with All Historical Data (Safest)\n",
    "\n",
    "Train on ALL data from 1947-present. Slower but most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full_retrain"
   },
   "outputs": [],
   "source": [
    "# Full retrain (2-3 hours on GPU)\n",
    "!python train_auto.py \\\n",
    "  --dataset eoinamoore/historical-nba-data-and-player-box-scores \\\n",
    "  --verbose \\\n",
    "  --fresh \\\n",
    "  --enable-window-ensemble \\\n",
    "  --neural-device gpu\n",
    "\n",
    "print(\"\\n[OK] Full retrain complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "approach_b"
   },
   "source": [
    "### Approach B: Sliding Window (Faster - 30-60 min)\n",
    "\n",
    "Train only on recent data (e.g., last 3 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sliding_window"
   },
   "outputs": [],
   "source": [
    "# TODO: Create sliding window training script\n",
    "# For now, use full retrain or implement custom logic\n",
    "\n",
    "print(\"[TODO] Sliding window training - coming soon\")\n",
    "print(\"[INFO] Use full retrain for now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate"
   },
   "source": [
    "## Step 10: Validate New Models\n",
    "\n",
    "Test performance on recent games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation"
   },
   "outputs": [],
   "source": [
    "# Validate on last 2 weeks of games\n",
    "validation_cutoff = datetime.datetime.now() - datetime.timedelta(days=14)\n",
    "validation_games = games_df[games_df['GAME_DATE_EST'] >= validation_cutoff]\n",
    "\n",
    "print(f\"[INFO] Validation set: {len(validation_games)} games from last 14 days\")\n",
    "\n",
    "# Run predictions (using your existing prediction script)\n",
    "# !python predict.py --validate\n",
    "\n",
    "print(\"[TODO] Add validation script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## Step 11: Save Updated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_models"
   },
   "outputs": [],
   "source": [
    "# Models are already saved by train_auto.py to model_cache/\n",
    "# Copy to versioned directory\n",
    "\n",
    "import shutil\n",
    "\n",
    "version = UPDATE_CONFIG['version']\n",
    "version_dir = f\"models_v{version}\"\n",
    "os.makedirs(version_dir, exist_ok=True)\n",
    "\n",
    "# Copy all model files\n",
    "!cp -r model_cache/* {version_dir}/\n",
    "\n",
    "print(f\"[OK] Models saved to {version_dir}/\")\n",
    "print(f\"\\n[INFO] Model files:\")\n",
    "!ls -lh {version_dir}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_models"
   },
   "source": [
    "## Step 12: Download Updated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_updated"
   },
   "outputs": [],
   "source": [
    "# Zip and download\n",
    "!zip -r models_v{UPDATE_CONFIG['version']}.zip {version_dir}/\n",
    "\n",
    "from google.colab import files\n",
    "files.download(f\"models_v{UPDATE_CONFIG['version']}.zip\")\n",
    "\n",
    "print(f\"[OK] Models downloaded: models_v{UPDATE_CONFIG['version']}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You Just Did:\n",
    "1. ✅ Downloaded latest NBA data from Kaggle\n",
    "2. ✅ Identified new games since last update\n",
    "3. ✅ Trained updated models (full retrain or incremental)\n",
    "4. ✅ Validated performance\n",
    "5. ✅ Saved versioned models\n",
    "\n",
    "### Monthly Workflow:\n",
    "- **Day 1 of month:** Run this notebook\n",
    "- **Weekly:** Run recalibration (separate notebook)\n",
    "- **As needed:** Update predictions for upcoming games\n",
    "\n",
    "### Model Versions:\n",
    "- Current: v{UPDATE_CONFIG['version']}\n",
    "- Keep last 3 months of models for rollback\n",
    "\n",
    "### Performance Monitoring:\n",
    "Track these metrics monthly:\n",
    "- MAE on validation set\n",
    "- Calibration error\n",
    "- Hit rate on over/under predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Weekly Recalibration\n",
    "\n",
    "Between monthly updates, run weekly recalibration to adjust predictions without full retrain.\n",
    "\n",
    "See: `weekly_recalibration.ipynb` (coming next)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
