{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# TabNet Embedding Dimension Test (FIXED)\n",
    "\n",
    "**Goal:** Verify that TabNet is producing proper compressed embeddings\n",
    "\n",
    "**Expected:** 24 dimensions (sweet spot for learned representations)\n",
    "\n",
    "**NOT:** 150 dimensions (raw features) or 1 dimension (predictions only)\n",
    "\n",
    "**Runtime:** ~1-2 minutes on Colab GPU\n",
    "\n",
    "---\n",
    "\n",
    "## What We Fixed:\n",
    "- ‚ùå Old method: Got 150-dim from `forward_masks` (just transformed inputs)\n",
    "- ‚úÖ New method: Get 24-48 dim from `encoder` (compressed learned embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install -q pytorch-tabnet lightgbm scikit-learn pandas numpy torch\n",
    "print(\"[OK] All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_gpu"
   },
   "source": [
    "## Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[OK] GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    print(\"[WARNING] No GPU found, using CPU (will be slower)\")\n",
    "    USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_files"
   },
   "source": [
    "## Step 3: Upload FIXED neural_hybrid.py\n",
    "\n",
    "**IMPORTANT:** Make sure you upload the LATEST version with the fixed `_get_embeddings()` method!\n",
    "\n",
    "**Click the folder icon on the left ‚Üí Upload ‚Üí Select `neural_hybrid.py`**\n",
    "\n",
    "Then run this cell to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_file"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('neural_hybrid.py'):\n",
    "    print(\"[OK] neural_hybrid.py found\")\n",
    "    \n",
    "    # Check if it has the fix\n",
    "    with open('neural_hybrid.py', 'r') as f:\n",
    "        content = f.read()\n",
    "        if 'steps_output[:, -1, :].cpu().numpy()' in content:\n",
    "            print(\"[OK] File contains the FIXED embedding extraction method\")\n",
    "        else:\n",
    "            print(\"[WARNING] File might be the OLD version - make sure you uploaded the latest!\")\n",
    "else:\n",
    "    print(\"[ERROR] neural_hybrid.py NOT found\")\n",
    "    print(\"   Please upload it using the file browser on the left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_embeddings"
   },
   "source": [
    "## Step 4: Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_data"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neural_hybrid import NeuralHybridPredictor\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING TABNET EMBEDDING EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create dummy data (similar to your NBA stats)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_samples = 500\n",
    "n_features = 150  # Similar to your 150+ features\n",
    "\n",
    "X_train = pd.DataFrame(np.random.randn(n_samples, n_features))\n",
    "y_train = pd.Series(np.random.randn(n_samples) * 5 + 15)  # Simulate points/assists\n",
    "\n",
    "X_val = pd.DataFrame(np.random.randn(100, n_features))\n",
    "y_val = pd.Series(np.random.randn(100) * 5 + 15)\n",
    "\n",
    "X_test = pd.DataFrame(np.random.randn(20, n_features))\n",
    "\n",
    "print(f\"\\n[OK] Created test data:\")\n",
    "print(f\"  - Train: {n_samples} samples, {n_features} features\")\n",
    "print(f\"  - Val: {len(X_val)} samples\")\n",
    "print(f\"  - Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model"
   },
   "source": [
    "## Step 5: Initialize Model\n",
    "\n",
    "This shows the TabNet configuration:\n",
    "- **n_d = 24** (dimension for each decision step)\n",
    "- **n_a = 24** (attention dimension)\n",
    "- **n_steps = 4** (number of sequential decision steps)\n",
    "\n",
    "Expected embedding dimension: **24-48** (compressed learned representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_model"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = NeuralHybridPredictor(prop_name='test_points', use_gpu=USE_GPU)\n",
    "\n",
    "print(f\"\\n[OK] Initialized NeuralHybridPredictor\")\n",
    "print(f\"  - n_d (dimension): {model.tabnet_params['n_d']}\")\n",
    "print(f\"  - n_a (attention): {model.tabnet_params['n_a']}\")\n",
    "print(f\"  - n_steps: {model.tabnet_params['n_steps']}\")\n",
    "print(f\"  - Expected embedding size: {model.tabnet_params['n_d']} to {model.tabnet_params['n_d'] + model.tabnet_params['n_a']}\")\n",
    "print(f\"  - Device: {'GPU' if USE_GPU else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## Step 6: Train TabNet (1 Epoch Only)\n",
    "\n",
    "**Watch for the line:**\n",
    "```\n",
    "  - Embedding dimension: XX\n",
    "```\n",
    "\n",
    "This is printed during Step 2 of training and shows what dimension the embeddings actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_tabnet"
   },
   "outputs": [],
   "source": [
    "# Train with 1 epoch for quick test\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING TABNET (1 EPOCH)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"This will take ~30-60 seconds on GPU, ~2-3 minutes on CPU\\n\")\n",
    "\n",
    "model.fit(X_train, y_train, X_val, y_val, epochs=1, batch_size=256)\n",
    "\n",
    "print(f\"\\n[OK] Model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "extract"
   },
   "source": [
    "## Step 7: Extract Embeddings and Check Dimension\n",
    "\n",
    "**üëÄ WATCH FOR:**\n",
    "\n",
    "The line that says:\n",
    "```\n",
    "[INFO] Extracted XX-dim embeddings from encoder (last step)\n",
    "```\n",
    "\n",
    "**What the dimension means:**\n",
    "- **24 dimensions** ‚úÖ Perfect! Compressed learned embeddings (n_d)\n",
    "- **48 dimensions** ‚úÖ Good! Full encoder output (n_d + n_a)\n",
    "- **8-32 dimensions** ‚úÖ Acceptable sweet spot for embeddings\n",
    "- **150 dimensions** ‚ùå BAD - Just transformed inputs, no compression\n",
    "- **1 dimension** ‚ùå BAD - Fallback to predictions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_embeddings"
   },
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXTRACTING EMBEDDINGS FROM TEST DATA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nüëÄ Watch for [INFO] or [WARNING] messages\\n\")\n",
    "\n",
    "try:\n",
    "    embeddings = model._get_embeddings(X_test.values.astype(np.float32))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "    print(f\"Expected shape: (20, 24-48)\")\n",
    "    print(f\"\\n‚≠ê ACTUAL DIMENSION: {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    if 8 <= dim <= 48:\n",
    "        print(f\"\\n‚úÖ SUCCESS! Embeddings are {dim}-dimensional!\")\n",
    "        print(f\"\\nThis is in the optimal range (8-48 dimensions).\")\n",
    "        print(f\"Your TabNet is producing compressed learned representations.\")\n",
    "        \n",
    "        if dim == 24:\n",
    "            print(f\"\\nüéØ PERFECT! 24 dimensions is the sweet spot!\")\n",
    "        elif dim == 48:\n",
    "            print(f\"\\nüëç GOOD! 48 dimensions (n_d + n_a) captures full encoder output.\")\n",
    "        \n",
    "        print(f\"\\nSample embedding (first 5 values of row 0):\")\n",
    "        print(embeddings[0, :5])\n",
    "        print(f\"\\nEmbedding statistics:\")\n",
    "        print(f\"  - Mean: {embeddings.mean():.4f}\")\n",
    "        print(f\"  - Std: {embeddings.std():.4f}\")\n",
    "        print(f\"  - Min: {embeddings.min():.4f}\")\n",
    "        print(f\"  - Max: {embeddings.max():.4f}\")\n",
    "        \n",
    "    elif dim == 150:\n",
    "        print(f\"\\n‚ùå PROBLEM! Embeddings are {dim}-dimensional\")\n",
    "        print(f\"\\nThis equals your input feature count.\")\n",
    "        print(f\"TabNet is NOT compressing features into learned embeddings.\")\n",
    "        print(f\"It's just passing through transformed inputs.\")\n",
    "        print(f\"\\nüîß The _get_embeddings() method needs more fixes.\")\n",
    "        \n",
    "    elif dim == 1 or dim == 2:\n",
    "        print(f\"\\n‚ùå FAILED! Embeddings are only {dim}-dimensional\")\n",
    "        print(f\"\\nFallback code was used (predictions only).\")\n",
    "        print(f\"TabNet's internal encoder is not accessible.\")\n",
    "        print(f\"\\nüîß Check the error messages above.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è UNEXPECTED! Embeddings are {dim}-dimensional\")\n",
    "        print(f\"\\nExpected 24-48, got {dim}.\")\n",
    "        print(f\"This might still work, but it's not the expected range.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR extracting embeddings:\")\n",
    "    print(f\"   {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### If you got 24-48 dimensions:\n",
    "‚úÖ **PERFECT!** Your TabNet is working correctly!\n",
    "\n",
    "- TabNet is producing compressed learned embeddings\n",
    "- These capture complex non-linear relationships from your 150 features\n",
    "- Your hybrid ensemble (TabNet embeddings + LightGBM) is optimized\n",
    "\n",
    "**Next:** Your system is ready. Now you can optionally add H2O AutoML to discover additional feature interactions.\n",
    "\n",
    "---\n",
    "\n",
    "### If you got 150 dimensions:\n",
    "‚ùå **PROBLEM:** TabNet is not compressing features\n",
    "\n",
    "The code is extracting from `forward_masks` which returns transformed inputs, not encoder embeddings.\n",
    "\n",
    "**Next:** Need to debug the encoder access in `_get_embeddings()` method.\n",
    "\n",
    "---\n",
    "\n",
    "### If you got 1-2 dimensions:\n",
    "‚ùå **FAILED:** Fallback code was used\n",
    "\n",
    "The encoder extraction failed and fell back to using predictions only.\n",
    "\n",
    "**Next:** Check error messages and debug why encoder is not accessible.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Answer\n",
    "\n",
    "**Report back with:**\n",
    "```\n",
    "‚≠ê ACTUAL DIMENSION: XX\n",
    "```\n",
    "\n",
    "And I'll either:\n",
    "1. ‚úÖ Confirm your system is perfect and explain next steps for H2O AutoML\n",
    "2. üîß Give you the exact fix to get proper compressed embeddings"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
