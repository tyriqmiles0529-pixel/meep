#!/usr/bin/env python
"""
Meta-Learner V3 - Advanced NBA Prediction Architecture

V3 Components:
‚úÖ Cross-Stat Meta-Dependency Graph
‚úÖ Dynamic Regime Detection (Window Clustering)
‚úÖ Player Archetype Clustering  
‚úÖ Meta-Mixer Layer (Global Stacking)
‚úÖ Temporal Intelligence Upgrade
‚úÖ Monte Carlo Quantile Simulation
‚úÖ Calibration Layer for Uncertainty

Usage:
    python train_meta_learner_v3.py
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
from typing import Dict, List, Tuple, Optional
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.isotonic import IsotonicRegression
from scipy import stats
from scipy.stats import norm, beta
import warnings
warnings.filterwarnings('ignore')

# Configuration
TRAINING_SEASON = '2024-2025'
DATA_FILE = 'PlayerStatistics.csv'
OUTPUT_FILE = 'model_cache/meta_learner_v3_2025_2026.pkl'
MIN_SAMPLES_PER_PROP = 100

class CrossStatDependencyGraph:
    """
    Models dependencies between different stats:
    minutes ‚Üí points, assists, rebounds
    usage ‚Üí points, assists  
    pace ‚Üí assists, rebounds
    """
    
    def __init__(self):
        self.dependency_weights = {
            'points': {'minutes': 0.3, 'usage': 0.4, 'pace': 0.1},
            'assists': {'minutes': 0.4, 'usage': 0.3, 'pace': 0.2},
            'rebounds': {'minutes': 0.3, 'usage': 0.1, 'pace': 0.3},
            'threes': {'minutes': 0.2, 'usage': 0.5, 'pace': 0.1},
            'minutes': {'usage': 0.6, 'pace': 0.2}
        }
    
    def create_cross_stat_features(self, game_data: pd.Series, base_predictions: Dict[str, float]) -> Dict[str, float]:
        """Create cross-stat interaction features"""
        features = {}
        
        # Extract base predictions
        minutes_pred = base_predictions.get('minutes', 30.0)
        usage_rate = game_data.get('USG_PCT', 0.2)
        pace = game_data.get('OPP_PACE', 100.0)
        
        # Cross-stat dependencies
        for stat, deps in self.dependency_weights.items():
            interaction_score = 0.0
            
            if 'minutes' in deps:
                interaction_score += deps['minutes'] * (minutes_pred / 30.0)
            if 'usage' in deps:
                interaction_score += deps['usage'] * usage_rate
            if 'pace' in deps:
                interaction_score += deps['pace'] * (pace / 100.0)
            
            features[f'{stat}_cross_stat_dependency'] = interaction_score
        
        # Shared latent features across stats
        features['usage_efficiency_latent'] = usage_rate * game_data.get('TS_PCT', 0.5)
        features['pace_minutes_latent'] = (pace / 100.0) * (minutes_pred / 30.0)
        features['role_potential_latent'] = usage_rate * (minutes_pred / 30.0)
        
        return features

class DynamicRegimeDetector:
    """
    Dynamically clusters windows using PCA + KMeans instead of static era weights
    """
    
    def __init__(self, n_clusters: int = 5):
        self.n_clusters = n_clusters
        self.pca = PCA(n_components=3)
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        self.cluster_centers = None
        self.fitted = False
    
    def fit(self, window_predictions: np.ndarray) -> None:
        """Learn era clusters from window prediction patterns"""
        # Reduce dimensionality with PCA
        window_features = self.pca.fit_transform(window_predictions.T)
        
        # Cluster windows into regimes
        self.kmeans.fit(window_features)
        self.cluster_centers = self.kmeans.cluster_centers_
        self.fitted = True
        
        print(f"    ‚úì Discovered {self.n_clusters} dynamic regimes")
    
    def predict_regime_features(self, predictions: np.ndarray) -> Dict[str, float]:
        """Generate regime-based features for current predictions"""
        if not self.fitted:
            return {'regime_0': 1.0}  # Default if not fitted
        
        # Transform predictions to PCA space
        pred_features = self.pca.transform(predictions.reshape(1, -1))
        
        # Get cluster probabilities
        distances = np.linalg.norm(self.cluster_centers - pred_features, axis=1)
        probabilities = np.exp(-distances)
        probabilities = probabilities / probabilities.sum()
        
        features = {}
        for i, prob in enumerate(probabilities):
            features[f'regime_{i}_prob'] = prob
        
        features['dominant_regime'] = np.argmax(probabilities)
        features['regime_entropy'] = -np.sum(probabilities * np.log(probabilities + 1e-6))
        
        return features

class PlayerArchetypeClusterer:
    """
    Creates player clusters using usage, efficiency, shot profile, rebound profile
    """
    
    def __init__(self, n_archetypes: int = 8):
        self.n_archetypes = n_archetypes
        self.kmeans = KMeans(n_clusters=n_archetypes, random_state=42)
        self.scaler = StandardScaler()
        self.fitted = False
    
    def fit(self, player_stats: pd.DataFrame) -> None:
        """Learn player archetypes from historical data"""
        # Feature matrix for clustering
        features = pd.DataFrame()
        
        # Usage profile
        features['usage_rate'] = player_stats.get('USG_PCT', 0.2)
        features['minutes_per_game'] = player_stats.get('MIN', 30.0)
        
        # Efficiency profile
        features['true_shooting'] = player_stats.get('TS_PCT', 0.5)
        features['efficiency_rating'] = (player_stats.get('PTS', 0) + 
                                       player_stats.get('AST', 0) + 
                                       player_stats.get('REB', 0)) / (player_stats.get('MIN', 1) + 1e-6)
        
        # Shot profile
        features['three_point_rate'] = (player_stats.get('FG3M', 0) / 
                                      (player_stats.get('FGA', 1) + 1e-6))
        features['free_throw_rate'] = (player_stats.get('FTA', 0) / 
                                     (player_stats.get('FGA', 1) + 1e-6))
        
        # Rebound profile
        features['offensive_rebound_rate'] = (player_stats.get('OREB', 0) / 
                                            (player_stats.get('REB', 1) + 1e-6))
        features['defensive_rebound_rate'] = (player_stats.get('DREB', 0) / 
                                            (player_stats.get('REB', 1) + 1e-6))
        
        # Scale and cluster
        features_scaled = self.scaler.fit_transform(features.fillna(0))
        self.kmeans.fit(features_scaled)
        self.fitted = True
        
        print(f"    ‚úì Discovered {self.n_archetypes} player archetypes")
    
    def predict_archetype_features(self, game_data: pd.Series) -> Dict[str, float]:
        """Generate archetype-based features"""
        if not self.fitted:
            return {'archetype_0': 1.0}  # Default if not fitted
        
        # Create feature vector
        features = np.array([[
            game_data.get('USG_PCT', 0.2),
            game_data.get('MIN', 30.0),
            game_data.get('TS_PCT', 0.5),
            (game_data.get('PTS', 0) + game_data.get('AST', 0) + game_data.get('REB', 0)) / (game_data.get('MIN', 1) + 1e-6),
            game_data.get('FG3M', 0) / (game_data.get('FGA', 1) + 1e-6),
            game_data.get('FTA', 0) / (game_data.get('FGA', 1) + 1e-6),
            game_data.get('OREB', 0) / (game_data.get('REB', 1) + 1e-6),
            game_data.get('DREB', 0) / (game_data.get('REB', 1) + 1e-6)
        ]])
        
        # Scale and predict
        features_scaled = self.scaler.transform(features)
        distances = np.linalg.norm(self.kmeans.cluster_centers_ - features_scaled, axis=1)
        probabilities = np.exp(-distances)
        probabilities = probabilities / probabilities.sum()
        
        archetype_features = {}
        for i, prob in enumerate(probabilities):
            archetype_features[f'archetype_{i}_prob'] = prob
        
        archetype_features['dominant_archetype'] = np.argmax(probabilities)
        archetype_features['archetype_entropy'] = -np.sum(probabilities * np.log(probabilities + 1e-6))
        
        return archetype_features

class MetaMixerLayer:
    """
    Global stacking layer after stat-specific models
    Input: predictions from all props
    Output: refined predictions per prop
    """
    
    def __init__(self):
        self.mixer_models = {}
        self.fitted = False
    
    def fit(self, prop_predictions: Dict[str, np.ndarray], y_true: Dict[str, np.ndarray]) -> None:
        """Train global mixer models"""
        for target_prop in prop_predictions.keys():
            if target_prop not in y_true:
                continue
            
            # Create training data: all prop predictions as features
            X = []
            y = []
            
            for i in range(len(y_true[target_prop])):
                feature_row = []
                
                # Add predictions from all props
                for prop in sorted(prop_predictions.keys()):
                    if i < len(prop_predictions[prop]):
                        feature_row.append(prop_predictions[prop][i])
                    else:
                        feature_row.append(0.0)
                
                X.append(feature_row)
                y.append(y_true[target_prop][i])
            
            if len(X) > MIN_SAMPLES_PER_PROP:
                # Train mixer model
                mixer = GradientBoostingRegressor(
                    n_estimators=50,
                    max_depth=3,
                    learning_rate=0.1,
                    random_state=42
                )
                mixer.fit(X, y)
                self.mixer_models[target_prop] = mixer
                
                print(f"    ‚úì Trained meta-mixer for {target_prop}")
    
    def mix_predictions(self, prop_predictions: Dict[str, float]) -> Dict[str, float]:
        """Refine predictions using global mixing"""
        if not self.fitted:
            return prop_predictions
        
        mixed_predictions = {}
        
        for target_prop, mixer in self.mixer_models.items():
            # Create feature vector from all prop predictions
            feature_vector = [prop_predictions.get(prop, 0.0) for prop in sorted(self.mixer_models.keys())]
            
            # Get mixed prediction
            mixed_pred = mixer.predict([feature_vector])[0]
            mixed_predictions[target_prop] = mixed_pred
        
        return mixed_predictions

class TemporalIntelligence:
    """
    Advanced temporal features: exponential smoothing, streak strength, volatility, change-point detection
    """
    
    def __init__(self):
        self.change_point_models = {}
    
    def analyze_temporal_patterns(self, player_games: pd.DataFrame, prop_name: str) -> Dict[str, float]:
        """Generate advanced temporal features"""
        features = {}
        
        if len(player_games) < 5:
            # Default features for new players
            return {
                'exp_smooth_5': player_games[prop_name].iloc[-1] if len(player_games) > 0 else 0,
                'streak_strength': 0.0,
                'volatility_index': 0.1,
                'role_shift_detected': 0.0
            }
        
        # Exponential smoothing (last 5 games)
        recent_values = player_games[prop_name].tail(5).values
        alpha = 0.3  # Smoothing factor
        exp_smooth = recent_values[-1]
        for val in reversed(recent_values[:-1]):
            exp_smooth = alpha * val + (1 - alpha) * exp_smooth
        features['exp_smooth_5'] = exp_smooth
        
        # Streak strength (positive/negative)
        mean_val = np.mean(recent_values)
        positive_games = np.sum(recent_values > mean_val)
        streak_strength = (positive_games - len(recent_values) / 2) / (len(recent_values) / 2)
        features['streak_strength'] = streak_strength
        
        # Volatility index (rolling standard deviation)
        features['volatility_index'] = np.std(recent_values) / (np.mean(recent_values) + 1e-6)
        
        # Change-point detection (role shifts)
        if len(player_games) >= 10:
            # Simple change-point: compare recent 5 vs previous 5
            recent_mean = np.mean(player_games[prop_name].tail(5))
            previous_mean = np.mean(player_games[prop_name].iloc[-10:-5])
            
            change_magnitude = abs(recent_mean - previous_mean) / (previous_mean + 1e-6)
            features['role_shift_detected'] = min(change_magnitude, 2.0)  # Cap at 2.0
        else:
            features['role_shift_detected'] = 0.0
        
        return features

class MonteCarloSimulator:
    """
    Simulate stat outcomes based on predicted distribution
    Returns median, 25th/75th percentiles, confidence intervals
    """
    
    def __init__(self, n_simulations: int = 1000):
        self.n_simulations = n_simulations
    
    def simulate_outcomes(self, prediction: float, uncertainty: float, 
                         stat_name: str) -> Dict[str, float]:
        """Generate distribution-based predictions"""
        
        # Choose distribution based on stat characteristics
        if stat_name in ['points', 'rebounds', 'assists']:
            # Normal distribution for counting stats
            simulated = np.random.normal(prediction, max(uncertainty, 0.1), self.n_simulations)
            simulated = np.maximum(simulated, 0)  # No negative values
            
        elif stat_name == 'threes':
            # Poisson-like distribution for threes
            lambda_param = max(prediction, 0.1)
            simulated = np.random.poisson(lambda_param, self.n_simulations)
            
        elif stat_name == 'minutes':
            # Beta distribution for minutes (bounded)
            # Normalize to 0-48 minutes
            alpha = max(prediction / 10, 0.5)
            beta = max((48 - prediction) / 10, 0.5)
            simulated = np.random.beta(alpha, beta, self.n_simulations) * 48
        
        # Calculate quantiles
        results = {
            'median': np.median(simulated),
            'p25': np.percentile(simulated, 25),
            'p75': np.percentile(simulated, 75),
            'p10': np.percentile(simulated, 10),
            'p90': np.percentile(simulated, 90),
            'confidence_width': np.percentile(simulated, 90) - np.percentile(simulated, 10),
            'std_dev': np.std(simulated)
        }
        
        return results

class UncertaintyCalibrator:
    """
    Calibrate uncertainty predictions using isotonic regression
    Ensures predicted confidence matches real-world error
    """
    
    def __init__(self):
        self.calibrators = {}
        self.fitted = False
    
    def fit(self, predicted_uncertainty: np.ndarray, actual_errors: np.ndarray, 
            prop_name: str) -> None:
        """Train calibration model for each prop"""
        # Bin predictions for stable calibration
        n_bins = 10
        bin_edges = np.percentile(predicted_uncertainty, np.linspace(0, 100, n_bins + 1))
        bin_indices = np.digitize(predicted_uncertainty, bin_edges[:-1])
        
        # Calculate actual error per bin
        calibrated_uncertainty = np.zeros_like(predicted_uncertainty)
        for i in range(n_bins):
            mask = bin_indices == i
            if np.sum(mask) > 5:  # Need enough samples per bin
                actual_error = np.mean(actual_errors[mask])
                calibrated_uncertainty[mask] = actual_error
        
        # Fit isotonic regression
        iso_reg = IsotonicRegression(out_of_bounds='clip')
        iso_reg.fit(predicted_uncertainty, calibrated_uncertainty)
        self.calibrators[prop_name] = iso_reg
        self.fitted = True
        
        print(f"    ‚úì Calibrated uncertainty for {prop_name}")
    
    def calibrate(self, uncertainty: float, prop_name: str) -> float:
        """Apply calibration to uncertainty prediction"""
        if not self.fitted or prop_name not in self.calibrators:
            return uncertainty
        
        return self.calibrators[prop_name].predict([uncertainty])[0]

class MetaLearnerV3:
    """
    Complete V3 architecture with all advanced components
    """
    
    def __init__(self, n_windows: int = 27):
        self.n_windows = n_windows
        
        # V3 Components
        self.cross_stat_graph = CrossStatDependencyGraph()
        self.regime_detector = DynamicRegimeDetector()
        self.archetype_clusterer = PlayerArchetypeClusterer()
        self.meta_mixer = MetaMixerLayer()
        self.temporal_intel = TemporalIntelligence()
        self.monte_carlo = MonteCarloSimulator()
        self.uncertainty_calibrator = UncertaintyCalibrator()
        
        # Base models
        self.meta_models = {}
        self.linear_models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.uncertainty_models = {}
        self.feature_importance = {}
        
    def fit_v3_components(self, window_predictions: Dict[str, np.ndarray], 
                          games_df: pd.DataFrame, player_stats: pd.DataFrame) -> None:
        """Fit all V3 components"""
        print("  Fitting V3 components...")
        
        # Fit dynamic regime detector
        all_predictions = np.vstack([window_predictions[prop] for prop in window_predictions.keys()])
        self.regime_detector.fit(all_predictions)
        
        # Fit player archetype clusterer
        self.archetype_clusterer.fit(player_stats)
        
        print("  ‚úì V3 components fitted")
    
    def generate_v3_features(self, game_data: pd.Series, 
                            window_predictions: Dict[str, float],
                            player_games: pd.DataFrame) -> pd.DataFrame:
        """Generate complete V3 feature set"""
        features = {}
        
        # 1. Cross-stat dependency features
        cross_stat_features = self.cross_stat_graph.create_cross_stat_features(
            game_data, window_predictions
        )
        features.update(cross_stat_features)
        
        # 2. Dynamic regime features
        pred_array = np.array(list(window_predictions.values()))
        regime_features = self.regime_detector.predict_regime_features(pred_array)
        features.update(regime_features)
        
        # 3. Player archetype features
        archetype_features = self.archetype_clusterer.predict_archetype_features(game_data)
        features.update(archetype_features)
        
        # 4. Temporal intelligence features
        for prop in ['points', 'assists', 'rebounds', 'threes', 'minutes']:
            if prop in player_games.columns:
                temporal_features = self.temporal_intel.analyze_temporal_patterns(
                    player_games, prop
                )
                for key, value in temporal_features.items():
                    features[f'{prop}_{key}'] = value
        
        # 5. Base prediction features
        for prop, pred in window_predictions.items():
            features[f'{prop}_base_pred'] = pred
            features[f'{prop}_pred_uncertainty'] = pred * 0.1  # Placeholder
        
        return pd.DataFrame([features])
    
    def fit_oof_v3(self, window_predictions: Dict[str, np.ndarray], 
                   y_true: Dict[str, np.ndarray], games_df: pd.DataFrame,
                   player_stats: pd.DataFrame) -> Dict[str, Dict]:
        """V3 training with all components"""
        print("  Training V3 meta-learner with full architecture...")
        
        # Fit V3 components first
        self.fit_v3_components(window_predictions, games_df, player_stats)
        
        # Train stat-specific models
        all_metrics = {}
        prop_predictions = {}
        
        for prop_name in window_predictions.keys():
            if prop_name not in y_true:
                continue
            
            print(f"    Training {prop_name} model...")
            
            # Generate V3 features for all games
            X_list = []
            y_list = []
            
            for i in range(len(y_true[prop_name])):
                if i >= len(games_df):
                    break
                
                game_data = games_df.iloc[i]
                player_id = game_data.get('playerId')
                
                # Get player history for temporal features
                player_history = player_stats[player_stats['playerId'] == player_id].tail(10)
                
                # Create window predictions dict
                current_preds = {prop: window_predictions[prop][i] 
                                for prop in window_predictions.keys() 
                                if i < len(window_predictions[prop])}
                
                # Generate V3 features
                features = self.generate_v3_features(game_data, current_preds, player_history)
                X_list.append(features.iloc[0])
                y_list.append(y_true[prop_name][i])
            
            if len(X_list) < MIN_SAMPLES_PER_PROP:
                continue
            
            X = pd.DataFrame(X_list)
            y = np.array(y_list)
            
            # Feature selection and scaling
            selector = SelectKBest(score_func=f_regression, k=min(40, len(X.columns)))
            X_selected = selector.fit_transform(X, y)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_selected)
            
            # Train models
            nonlinear_model = GradientBoostingRegressor(
                n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42
            )
            linear_model = Ridge(alpha=1.0, random_state=42)
            
            nonlinear_model.fit(X_scaled, y)
            linear_model.fit(X_scaled, y)
            
            # Store models
            self.meta_models[prop_name] = nonlinear_model
            self.linear_models[prop_name] = linear_model
            self.scalers[prop_name] = scaler
            self.feature_selectors[prop_name] = selector
            
            # Collect predictions for meta-mixer
            ensemble_pred = 0.7 * nonlinear_model.predict(X_scaled) + 0.3 * linear_model.predict(X_scaled)
            prop_predictions[prop_name] = ensemble_pred
            
            # Calculate metrics
            baseline_pred = np.mean([np.mean(window_predictions[prop_name]) for _ in range(len(y))])
            baseline_mae = mean_absolute_error(y, baseline_pred)
            meta_mae = mean_absolute_error(y, ensemble_pred)
            improvement = ((baseline_mae - meta_mae) / baseline_mae) * 100
            
            all_metrics[prop_name] = {
                'mae': meta_mae,
                'improvement_mae_pct': improvement
            }
            
            print(f"      ‚úì {prop_name}: MAE improvement {improvement:+.1f}%")
        
        # Train meta-mixer layer
        print("  Training meta-mixer layer...")
        self.meta_mixer.fit(prop_predictions, y_true)
        self.meta_mixer.fitted = True
        
        return all_metrics
    
    def predict_v3(self, game_data: pd.Series, window_predictions: Dict[str, float],
                   player_games: pd.DataFrame) -> Dict[str, Dict]:
        """Complete V3 prediction with Monte Carlo and calibration"""
        
        # Generate V3 features
        features = self.generate_v3_features(game_data, window_predictions, player_games)
        
        # Get base predictions
        base_predictions = {}
        
        for prop_name in self.meta_models.keys():
            if prop_name in window_predictions:
                # Feature selection and scaling
                X_selected = self.feature_selectors[prop_name].transform(features)
                X_scaled = self.scalers[prop_name].transform(X_selected)
                
                # Ensemble prediction
                nonlinear_pred = self.meta_models[prop_name].predict(X_scaled)[0]
                linear_pred = self.linear_models[prop_name].predict(X_scaled)[0]
                ensemble_pred = 0.7 * nonlinear_pred + 0.3 * linear_pred
                
                base_predictions[prop_name] = ensemble_pred
        
        # Apply meta-mixer refinement
        if self.meta_mixer.fitted:
            mixed_predictions = self.meta_mixer.mix_predictions(base_predictions)
        else:
            mixed_predictions = base_predictions
        
        # Generate Monte Carlo simulations and calibrated uncertainty
        final_predictions = {}
        
        for prop_name, pred in mixed_predictions.items():
            # Get uncertainty estimate
            uncertainty = pred * 0.15  # Base uncertainty (15% of prediction)
            
            # Calibrate uncertainty
            calibrated_uncertainty = self.uncertainty_calibrator.calibrate(uncertainty, prop_name)
            
            # Monte Carlo simulation
            mc_results = self.monte_carlo.simulate_outcomes(pred, calibrated_uncertainty, prop_name)
            
            final_predictions[prop_name] = {
                'prediction': pred,
                'median': mc_results['median'],
                'p25': mc_results['p25'],
                'p75': mc_results['p75'],
                'p10': mc_results['p10'],
                'p90': mc_results['p90'],
                'confidence_width': mc_results['confidence_width'],
                'calibrated_uncertainty': calibrated_uncertainty
            }
        
        return final_predictions
    
    def save(self, path: str):
        """Save complete V3 architecture"""
        save_data = {
            # V3 Components
            'cross_stat_graph': self.cross_stat_graph,
            'regime_detector': self.regime_detector,
            'archetype_clusterer': self.archetype_clusterer,
            'meta_mixer': self.meta_mixer,
            'temporal_intel': self.temporal_intel,
            'monte_carlo': self.monte_carlo,
            'uncertainty_calibrator': self.uncertainty_calibrator,
            
            # Base models
            'meta_models': self.meta_models,
            'linear_models': self.linear_models,
            'scalers': self.scalers,
            'feature_selectors': self.feature_selectors,
            'uncertainty_models': self.uncertainty_models,
            'feature_importance': self.feature_importance,
            'n_windows': self.n_windows
        }
        
        with open(path, 'wb') as f:
            pickle.dump(save_data, f)
    
    def load(self, path: str):
        """Load complete V3 architecture"""
        with open(path, 'rb') as f:
            save_data = pickle.load(f)
        
        # Load V3 components
        self.cross_stat_graph = save_data['cross_stat_graph']
        self.regime_detector = save_data['regime_detector']
        self.archetype_clusterer = save_data['archetype_clusterer']
        self.meta_mixer = save_data['meta_mixer']
        self.temporal_intel = save_data['temporal_intel']
        self.monte_carlo = save_data['monte_carlo']
        self.uncertainty_calibrator = save_data['uncertainty_calibrator']
        
        # Load base models
        self.meta_models = save_data['meta_models']
        self.linear_models = save_data['linear_models']
        self.scalers = save_data['scalers']
        self.feature_selectors = save_data['feature_selectors']
        self.uncertainty_models = save_data['uncertainty_models']
        self.feature_importance = save_data['feature_importance']
        self.n_windows = save_data['n_windows']


# Data loading functions (reuse from previous versions)
def load_player_statistics_csv(csv_path: str = 'PlayerStatistics.csv') -> pd.DataFrame:
    if not Path(csv_path).exists():
        raise FileNotFoundError(f"PlayerStatistics.csv not found at {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"Loaded {len(df):,} rows from {csv_path}")
    return df


def filter_to_season(df: pd.DataFrame, season: str = '2024-2025') -> pd.DataFrame:
    if 'gameDate' not in df.columns:
        raise ValueError("gameDate column required for season filtering")
    
    df['gameDate'] = pd.to_datetime(df['gameDate'], errors='coerce')
    
    if season == '2024-2025':
        start_date = pd.to_datetime('2024-10-01')
        end_date = pd.to_datetime('2025-06-30')
    else:
        start_year = int(season.split('-')[0])
        start_date = pd.to_datetime(f'{start_year}-10-01')
        end_date = pd.to_datetime(f'{start_year+1}-06-30')
    
    season_df = df[
        (df['gameDate'] >= start_date) & (df['gameDate'] <= end_date)
    ].copy().reset_index(drop=True)
    
    print(f"Filtered to {season}: {len(season_df):,} rows")
    return season_df


def collect_window_predictions_v3(games_df: pd.DataFrame, window_models: Dict, 
                                  player_stats: pd.DataFrame) -> Dict:
    """Collect window predictions with V3 context"""
    print("  Collecting window predictions for V3 training...")
    
    window_predictions = {prop: [] for prop in ['points', 'rebounds', 'assists', 'threes', 'minutes']}
    actuals = {prop: [] for prop in ['points', 'rebounds', 'assists', 'threes', 'minutes']}
    
    prop_col_map = {
        'points': 'PTS', 'rebounds': 'REB', 'assists': 'AST', 
        'threes': 'FG3M', 'minutes': 'MIN'
    }
    
    for idx, game in games_df.iterrows():
        game_predictions = {}
        
        # Get predictions from each window
        for prop in prop_col_map.keys():
            preds_for_prop = []
            
            for window_name, models in window_models.items():
                try:
                    # Create features for window model
                    X_game = pd.DataFrame([{
                        'fieldGoalsAttempted': game.get('FGA', 0),
                        'freeThrowsAttempted': game.get('FTA', 0),
                        'assists': game.get('AST', 0),
                        'reboundsTotal': game.get('REB', 0),
                        'threePointersMade': game.get('FG3M', 0),
                        'points': game.get('PTS', 0),
                        'numMinutes': game.get('MIN', 0),
                        'fieldGoalsMade': game.get('FGM', 0),
                        'freeThrowsMade': game.get('FTM', 0),
                        'turnovers': game.get('TOV', 0),
                        'steals': game.get('STL', 0),
                        'blocks': game.get('BLK', 0),
                        'reboundsDefensive': game.get('DREB', 0),
                        'reboundsOffensive': game.get('OREB', 0),
                    }])
                    
                    from ensemble_predictor import predict_with_window
                    pred = predict_with_window(models, X_game, prop)
                    if pred is not None:
                        preds_for_prop.append(pred)
                    else:
                        preds_for_prop.append(0.0)
                except:
                    preds_for_prop.append(0.0)
            
            if len(preds_for_prop) >= 20:
                window_predictions[prop].append(preds_for_prop)
                
                # Get actual
                actual = game.get(prop_col_map[prop])
                if not pd.isna(actual) and actual >= 0:
                    actuals[prop].append(actual)
        
        # Ensure all props have same number of samples
        min_samples = min(len(actuals[prop]) for prop in actuals.keys() if len(actuals[prop]) > 0)
        if min_samples >= MIN_SAMPLES_PER_PROP:
            for prop in window_predictions.keys():
                window_predictions[prop] = window_predictions[prop][:min_samples]
                actuals[prop] = actuals[prop][:min_samples]
    
    # Convert to arrays
    for prop in window_predictions.keys():
        if len(window_predictions[prop]) > 0:
            window_predictions[prop] = np.array(window_predictions[prop])
            actuals[prop] = np.array(actuals[prop])
    
    print(f"  ‚úì Collected predictions for {len([p for p in actuals.values() if len(p) > 0])} props")
    
    return {
        'window_predictions': window_predictions,
        'actuals': actuals
    }


def train_meta_learner_v3():
    """Main V3 training function"""
    print(f"\n{'='*80}")
    print(f"META-LEARNER V3 - COMPLETE ADVANCED ARCHITECTURE")
    print(f"{'='*80}")
    print(f"  Training Season: {TRAINING_SEASON}")
    print(f"  Output: {OUTPUT_FILE}")
    print(f"  V3 Components:")
    print(f"    ‚úÖ Cross-Stat Meta-Dependency Graph")
    print(f"    ‚úÖ Dynamic Regime Detection (Window Clustering)")
    print(f"    ‚úÖ Player Archetype Clustering")
    print(f"    ‚úÖ Meta-Mixer Layer (Global Stacking)")
    print(f"    ‚úÖ Temporal Intelligence Upgrade")
    print(f"    ‚úÖ Monte Carlo Quantile Simulation")
    print(f"    ‚úÖ Calibration Layer for Uncertainty")
    print(f"{'='*80}\n")
    
    # Load window models
    print("Loading window models...")
    try:
        from ensemble_predictor import load_all_window_models
        window_models = load_all_window_models('model_cache')
        print(f"  ‚úì Loaded {len(window_models)} windows")
    except:
        print("  ! Window models not found - update when CPU models ready")
        return
    
    # Load training data
    print("Loading training data...")
    games_df = load_player_statistics_csv(DATA_FILE)
    games_df = filter_to_season(games_df, TRAINING_SEASON)
    player_stats = games_df.copy()  # For archetype clustering
    
    # Collect window predictions
    data = collect_window_predictions_v3(games_df, window_models, player_stats)
    
    # Initialize V3 meta-learner
    meta_learner = MetaLearnerV3(n_windows=len(window_models))
    
    # Train V3 architecture
    metrics = meta_learner.fit_oof_v3(
        data['window_predictions'],
        data['actuals'],
        games_df,
        player_stats
    )
    
    # Save V3 meta-learner
    output_path = Path(OUTPUT_FILE)
    output_path.parent.mkdir(exist_ok=True)
    meta_learner.save(str(output_path))
    
    print(f"\n{'='*80}")
    print(f"‚úÖ V3 TRAINING COMPLETE - ADVANCED ARCHITECTURE DEPLOYED")
    print(f"{'='*80}")
    print(f"  Saved to: {output_path}")
    print(f"  Components trained: {len(meta_learner.meta_models)}")
    
    for prop, prop_metrics in metrics.items():
        print(f"    {prop}: {prop_metrics['improvement_mae_pct']:+.1f}% MAE improvement")
    
    print(f"\nV3 Architecture Ready:")
    print(f"  üß† Cross-stat dependencies modeled")
    print(f"  üéØ Dynamic regime detection active")
    print(f"  üë• Player archetype clustering enabled")
    print(f"  üîÑ Meta-mixer stacking layer operational")
    print(f"  ‚è∞ Advanced temporal intelligence deployed")
    print(f"  üé≤ Monte Carlo quantile simulation ready")
    print(f"  üéØ Calibrated uncertainty estimates")
    print(f"\nReady for cutting-edge backtesting!")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    try:
        train_meta_learner_v3()
    except Exception as e:
        print(f"\n‚ùå V3 Training failed: {e}")
        import traceback
        traceback.print_exc()
